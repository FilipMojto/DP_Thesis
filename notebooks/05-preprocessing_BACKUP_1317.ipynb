{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262d8bc5",
   "metadata": {},
   "source": [
    "# 05 - Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a9bacf",
   "metadata": {},
   "source": [
    "## 5.1 - Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63351fa",
   "metadata": {},
   "source": [
    "### 5.1.1 - Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f4ebf446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging_config import setup_notebook_logging\n",
    "\n",
    "logger, log_start, log_check, log_result = setup_notebook_logging(label=\"PREPROCESSING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39586ea7",
   "metadata": {},
   "source": [
    "### 5.1.2 Configuring Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7596c094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PREPROCESSING CHECK] Setting up root by appending the parent to the sys...\n"
     ]
    }
   ],
   "source": [
    "log_check(\"Setting up root by appending the parent to the sys...\", print_to_console=True)\n",
    "from jupyter_init import setup\n",
    "\n",
    "setup()\n",
    "\n",
    "from src_code.config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5106823f",
   "metadata": {},
   "source": [
    "### 5.1.3 Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a551fc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PREPROCESSING CHECK] Loading the dataset...\n",
      "C:\\Users\\fmojt\\Code\\DPThesis\\DP_Thesis\\data\\interim\\test_labeled_features_partial_v10.feather\n",
      "[PREPROCESSING RESULT] Loaded dataframe with 7363 rows and 31 columns\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_check(\"Loading the dataset...\", print_to_console=True)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# TARGET_DF_FILE = ETL_MAPPINGS['test']['current_newest']\n",
    "subset: SubsetType = 'test'\n",
    "TARGET_DF_FILE = PREPROCESSING_MAPPINGS[subset]['input']\n",
    "print(ETL_MAPPINGS[subset]['current_newest'])\n",
    "\n",
    "# ---- LOAD ----\n",
    "df = pd.read_feather(TARGET_DF_FILE)\n",
    "log_result(f\"Loaded dataframe with {len(df)} rows and {len(df.columns)} columns\\n\", print_to_console=True)\n",
    "\n",
    "# For large datasets\n",
    "pd.set_option('display.max_columns', 50)\n",
    "sns.set_theme(style=\"whitegrid\", context=\"notebook\", palette=\"muted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9963d811",
   "metadata": {},
   "source": [
    "## 5.2 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9c15062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# preprocessor = ColumnTransformer(transformers=[])\n",
    "# append a transformer tuple (name, transformer, columns)\n",
    "# preprocessor.transformers.append(('new_passthrough', 'passthrough', ['col1', 'col2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b4a61",
   "metadata": {},
   "source": [
    "### 5.2.1 Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c477e8eb",
   "metadata": {},
   "source": [
    "#### 5.2.1.1 - Fix negative values before log transform\n",
    "Some features (e.g., time_since_last_change) contain negative values.\n",
    "\n",
    "We shift them to be ≥ 0 before applying log1p:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 69,
>>>>>>> fd17944a95fad7474e56d61eca4472c99072ef6e
   "id": "d73d379f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 0 rows with negative time_since_last_change\n"
     ]
    }
   ],
   "source": [
    "# def shift_min_to_zero(df, col):\n",
    "#     \"\"\"Shift column so minimum is 0 if negative values exist.\"\"\"\n",
    "#     min_val = df[col].min()\n",
    "#     if min_val < 0:\n",
    "#         df[col] = df[col] - min_val\n",
    "#     return df\n",
    "\n",
    "# for col in NUMERIC_FEATURES:\n",
    "#     df = shift_min_to_zero(df, col)\n",
    "\n",
<<<<<<< HEAD
    "from src_code.ml_pipeline.utils import contains_negative\n",
=======
    "from notebooks.utils import contains_negative\n",
>>>>>>> fd17944a95fad7474e56d61eca4472c99072ef6e
    "from notebooks.constants import NUMERIC_FEATURES\n",
    "\n",
    "NEG_FEATURES_TO_DROP = ['time_since_last_change']\n",
    "\n",
    "# List of features to check: NUMERIC_FEATURES excluding NEG_FEATURES_TO_DROP\n",
    "features_to_check = [\n",
    "    col for col in NUMERIC_FEATURES \n",
    "    if col not in NEG_FEATURES_TO_DROP\n",
    "]\n",
    "\n",
    "# Check if any of the features in features_to_check contain negative values\n",
    "if any(contains_negative(df, col) for col in features_to_check):\n",
    "    # If True, raise an exception\n",
    "    raise ValueError(\"Unexpected negative values found in one or more numeric features that are NOT set to be dropped.\")\n",
    "\n",
    "\n",
    "neg_mask = df[\"time_since_last_change\"] < 0\n",
    "n_neg = neg_mask.sum()\n",
    "\n",
    "print(f\"Dropping {n_neg} rows with negative time_since_last_change\")\n",
    "\n",
    "df = df[~neg_mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219ac218",
   "metadata": {},
   "source": [
    "#### 5.2.1.2 Assertion Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "38b75e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PREPROCESSING CHECK] [NEG_FEATURES_TO_DROP] Performing assertion check...\n",
      "[PREPROCESSING RESULT] [NEG_FEATURES_TO_DROP] Check succesfull!\n"
     ]
    }
   ],
   "source": [
    "log_check(\"[NEG_FEATURES_TO_DROP] Performing assertion check...\")\n",
    "assert(any(contains_negative(df, col) for col in NEG_FEATURES_TO_DROP) == False)\n",
    "log_result(\"[NEG_FEATURES_TO_DROP] Check succesfull!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563eeb35",
   "metadata": {},
   "source": [
    "### 5.2.2 Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03ae082",
   "metadata": {},
   "source": [
    "#### 5.2.2.1 - Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "85379de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c8f4ea",
   "metadata": {},
   "source": [
    "#### 5.2.2.2 - Transformations\n",
    "\n",
    "Applies the same bounds your EDA used.\n",
    "\n",
    "You want preprocessing to match your EDA findings, so we clamp values to the lower/upper fences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c73e8a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_embedding(df, col_name, prefix):\n",
    "    # Converts a column of arrays into a matrix\n",
    "    emb = np.vstack(df[col_name].values)\n",
    "    emb_df = pd.DataFrame(\n",
    "        emb,\n",
    "        index=df.index,\n",
    "        columns=[f\"{prefix}_{i}\" for i in range(emb.shape[1])]\n",
    "    )\n",
    "    return emb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4b487378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PREPROCESSING CHECK] Detected test subset. Loading fitted preprocessor...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PREPROCESSING RESULT] Transformations applied successfully.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.pipeline import FunctionTransformer, Pipeline\n",
    "from notebooks.transformers import EmbeddingExpander, NamingPCA, WinsorizerIQR\n",
    "from notebooks.constants import EMBEDDINGS, NUMERIC_FEATURES, LINE_TOKEN_FEATURES\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(transform_output='pandas')\n",
    "log_transformer = FunctionTransformer(np.log1p, validate=False)\n",
    "\n",
    "if subset == 'train':\n",
    "    # log_check(\"Detected train subset. Creating new preprocessor...\", print_to_console=True)\n",
    "    # preprocessor = ColumnTransformer(transformers=[], remainder='passthrough', verbose_feature_names_out=False)\n",
    "\n",
    "    # preprocessor.transformers.append(('winsorize', WinsorizerIQR(factor=1.5), NUMERIC_FEATURES))\n",
    "    # preprocessor.transformers.append(('log_tokens', log_transformer, LINE_TOKEN_FEATURES))\n",
    "    # preprocessor.transformers.append(('log_numeric', log_transformer, NUMERIC_FEATURES))\n",
    "\n",
    "    # # 3. FIT the preprocessor ONLY on the training data\n",
    "    # preprocessor.fit(df)\n",
    "    # df = preprocessor.transform(df)\n",
    "\n",
    "    # # 4. SAVE the fitted preprocessor\n",
    "    # # The saved object contains all the calculated Q1, Q3 bounds.\n",
    "    # joblib.dump(preprocessor, FITTED_PREPROCESSOR)\n",
    "    log_check(\"Detected train subset. Creating new preprocessor...\", print_to_console=True)\n",
    "\n",
    "    # code_emb_df = expand_embedding(df, \"code_embed\", \"code_emb\")\n",
    "    # msg_emb_df  = expand_embedding(df, \"msg_embed\", \"msg_emb\")\n",
    "    # df = pd.concat([df.drop(columns=[\"code_embed\", \"msg_embed\"]), code_emb_df, msg_emb_df], axis=1)\n",
    "\n",
    "    # Update the EMBEDDINGS constant to reflect the NEW flattened column names\n",
    "    # FLATTENED_EMBEDDINGS = code_emb_df.columns.tolist() + msg_emb_df.columns.tolist()\n",
    "\n",
    "    # Define a pipeline for EACH embedding type\n",
    "    # code_emb_pipe = Pipeline([\n",
    "    #     ('expand', EmbeddingExpander(prefix=\"code_emb\")),\n",
    "    #     ('pca', PCA(n_components=100, random_state=RANDOM_STATE))\n",
    "    # ])\n",
    "    # Use it in your pipeline like this:\n",
    "    code_emb_pipe = Pipeline([\n",
    "        ('expand', EmbeddingExpander(prefix=\"code\")),\n",
    "        ('pca', NamingPCA(n_components=10, prefix=\"code_emb_\", random_state=RANDOM_STATE))\n",
    "    ])\n",
    "\n",
    "    msg_emb_pipe = Pipeline([\n",
    "        ('expand', EmbeddingExpander(prefix=\"msg\")),\n",
    "        # ('pca', PCA(n_components=100, random_state=RANDOM_STATE))\n",
    "        ('pca', NamingPCA(n_components=45, prefix=\"msg_emb_\", random_state=RANDOM_STATE))\n",
    "\n",
    "    ])\n",
    "\n",
    "    # 1. Define a pipeline for numeric features: Winsorize THEN Log\n",
    "    numeric_pipeline = Pipeline([\n",
    "        ('winsorize', WinsorizerIQR(factor=1.5)),\n",
    "        ('log', log_transformer),\n",
    "        (\"var_thresh\", VarianceThreshold(threshold=0.0))\n",
    "    ])\n",
    "\n",
    "    # embedding_transformer = Pipeline(steps=[\n",
    "    #     (\"pca\", PCA(n_components=100, random_state=RANDOM_STATE))\n",
    "    # ])\n",
    "\n",
    "\n",
    "    # 2. Setup the ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            # ('num_transformed', numeric_pipeline, NUMERIC_FEATURES),\n",
    "            # ('token_transformed', log_transformer, LINE_TOKEN_FEATURES),\n",
    "            # (\"embed\", embedding_transformer, FLATTENED_EMBEDDINGS),\n",
    "            ('num', numeric_pipeline, NUMERIC_FEATURES),\n",
    "            ('tokens', log_transformer, LINE_TOKEN_FEATURES),\n",
    "            ('code_embed', code_emb_pipe, ['code_embed']), # Pass as list\n",
    "            ('msg_embed', msg_emb_pipe, ['msg_embed']),    # Pass as list\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "        verbose_feature_names_out=False  # This now works because names are unique\n",
    "    )\n",
    "\n",
    "    # 3. FIT and TRANSFORM\n",
    "    preprocessor.fit(df)\n",
    "    df = preprocessor.transform(df)\n",
    "\n",
    "    # 4. SAVE\n",
    "    joblib.dump(preprocessor, FITTED_PREPROCESSOR)\n",
    "\n",
    "    # print(\"Fitted preprocessor saved to fitted_preprocessor.joblib\")\n",
    "elif subset in ('test', 'validate'):\n",
    "    log_check(\"Detected test subset. Loading fitted preprocessor...\", print_to_console=True)\n",
    "    loaded_preprocessor = joblib.load(FITTED_PREPROCESSOR)\n",
    "    df = loaded_preprocessor.transform(df)\n",
    "else:\n",
    "    msg = \"Unknown subset value!\"\n",
    "    logger.error(msg)\n",
    "    raise ValueError(msg)\n",
    "\n",
    "\n",
    "log_result(\"Transformations applied successfully.\", print_to_console=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# numeric_transformer = Pipeline(steps=[\n",
    "#     (\"var_thresh\", VarianceThreshold(threshold=0.0))\n",
    "# ])\n",
    "\n",
    "# embedding_transformer = Pipeline(steps=[\n",
    "#     (\"pca\", PCA(n_components=100, random_state=RANDOM_STATE))\n",
    "# ])\n",
    "\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         # (\"struct\", \"passthrough\", structured_features),\n",
    "#         (\"struct\", numeric_transformer, structured_features),\n",
    "#         (\"embed\", embedding_transformer, embedding_features),\n",
    "#     ],\n",
    "#     remainder=\"drop\"\n",
    "# )\n",
    "\n",
    "# def winsorize_iqr(df, col, preserve_original: bool = False):\n",
    "#     \"\"\"\n",
    "#     Caps extreme outliers using IQR fences.\n",
    "#     Keeps the distribution shape mostly intact.\n",
    "#     \"\"\"\n",
    "#     Q1 = df[col].quantile(0.25)\n",
    "#     Q3 = df[col].quantile(0.75)\n",
    "#     IQR = Q3 - Q1\n",
    "#     lower = Q1 - 1.5 * IQR\n",
    "#     upper = Q3 + 1.5 * IQR\n",
    "\n",
    "#     print(f\"Df len before winsorization ({col}): {len(df)}\")\n",
    "\n",
    "#     new_col_name = col + \"_winsorized\" if preserve_original else col\n",
    "\n",
    "#     df[new_col_name] = df[col].clip(lower=lower, upper=upper)\n",
    "#     print(f\"Df len before winsorization ({col}): {len(df)}\")\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "# --- Apply to all numeric columns ---\n",
    "# for col in NUMERIC_FEATURES:\n",
    "#     df = winsorize_iqr(df, col, preserve_original=True) if col == 'recent_churn' else winsorize_iqr(df, col)\n",
    "\n",
    "# for col in LINE_TOKEN_FEATURES:\n",
    "#     # df = winsorize_iqr(df, col)\n",
    "#     df[col] = np.log1p(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2b30dbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your 50 components explain 85.40% of the original code data.\n",
      "Your 50 components explain 82.61% of the original msg data.\n"
     ]
    }
   ],
   "source": [
    "# Access the PCA step from your fitted preprocessor\n",
    "# Assuming the step was named 'code_pca' in the ColumnTransformer\n",
    "pca_model = preprocessor.named_transformers_['code_embed'].named_steps['pca']\n",
    "total_variance = sum(pca_model.explained_variance_ratio_)\n",
    "\n",
    "print(f\"Your 50 components explain {total_variance:.2%} of the original code data.\")\n",
    "\n",
    "pca_model = preprocessor.named_transformers_['msg_embed'].named_steps['pca']\n",
    "total_variance = sum(pca_model.explained_variance_ratio_)\n",
    "print(f\"Your 50 components explain {total_variance:.2%} of the original msg data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e4cf37",
   "metadata": {},
   "source": [
    "<!-- ### 5.2.2 - Fix negative values before log transform\n",
    "Some features (e.g., time_since_last_change) contain negative values.\n",
    "\n",
    "We shift them to be ≥ 0 before applying log1p: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c0151761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def shift_min_to_zero(df, col):\n",
    "# #     \"\"\"Shift column so minimum is 0 if negative values exist.\"\"\"\n",
    "# #     min_val = df[col].min()\n",
    "# #     if min_val < 0:\n",
    "# #         df[col] = df[col] - min_val\n",
    "# #     return df\n",
    "\n",
    "# # for col in NUMERIC_FEATURES:\n",
    "# #     df = shift_min_to_zero(df, col)\n",
    "\n",
    "# from notebooks.utils import contains_negative\n",
    "# from notebooks.constants import NUMERIC_FEATURES\n",
    "\n",
    "# NEG_FEATURES_TO_DROP = ['time_since_last_change']\n",
    "\n",
    "# # List of features to check: NUMERIC_FEATURES excluding NEG_FEATURES_TO_DROP\n",
    "# features_to_check = [\n",
    "#     col for col in NUMERIC_FEATURES \n",
    "#     if col not in NEG_FEATURES_TO_DROP\n",
    "# ]\n",
    "\n",
    "# # Check if any of the features in features_to_check contain negative values\n",
    "# if any(contains_negative(df, col) for col in features_to_check):\n",
    "#     # If True, raise an exception\n",
    "#     raise ValueError(\"Unexpected negative values found in one or more numeric features that are NOT set to be dropped.\")\n",
    "\n",
    "\n",
    "# neg_mask = df[\"time_since_last_change\"] < 0\n",
    "# n_neg = neg_mask.sum()\n",
    "\n",
    "# print(f\"Dropping {n_neg} rows with negative time_since_last_change\")\n",
    "\n",
    "# df = df[~neg_mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ec505e",
   "metadata": {},
   "source": [
    "### 5.2.3 - Log1p Transformation\n",
    "Reduces heavy right-skew (your EDA showed skews up to 100+)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6892df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in NUMERIC_FEATURES:\n",
    "#     df[col] = np.log1p(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8a70b5",
   "metadata": {},
   "source": [
    "## 5.3. - Save preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7bbcbbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PREPROCESSING CHECK] Saving the preprocessed dataset...\n",
      "[PREPROCESSING RESULT] Preprocessed dataset saved to C:\\Users\\fmojt\\Code\\DPThesis\\DP_Thesis\\data\\processed\\test_preprocessed.feather\n"
     ]
    }
   ],
   "source": [
    "log_check(\"Saving the preprocessed dataset...\", print_to_console=True)\n",
    "\n",
    "OUTPUT_PATH = PREPROCESSING_MAPPINGS[subset]['output']\n",
    "\n",
    "# 1. Get the names of the final features\n",
    "# feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# 2. Reconstruct the DataFrame\n",
    "# df_transformed = pd.DataFrame(df, columns=feature_names)\n",
    "\n",
    "df.to_feather(OUTPUT_PATH)\n",
    "\n",
    "log_result(f\"Preprocessed dataset saved to {OUTPUT_PATH}\", print_to_console=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "30464f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['author_exp_pre', 'author_recent_activity_pre', 'loc_added', 'loc_deleted', 'files_changed', 'hunks_count', 'msg_len', 'ast_delta', 'complexity_delta', 'max_func_change', 'time_since_last_change', 'recent_churn', 'todo', 'fixme', 'try', 'except', 'raise', 'code_emb_0', 'code_emb_1', 'code_emb_2', 'code_emb_3', 'code_emb_4', 'code_emb_5', 'code_emb_6', 'code_emb_7', 'code_emb_8', 'code_emb_9', 'msg_emb_0', 'msg_emb_1', 'msg_emb_2', 'msg_emb_3', 'msg_emb_4', 'msg_emb_5', 'msg_emb_6', 'msg_emb_7', 'msg_emb_8', 'msg_emb_9', 'msg_emb_10', 'msg_emb_11', 'msg_emb_12', 'msg_emb_13', 'msg_emb_14', 'msg_emb_15', 'msg_emb_16', 'msg_emb_17', 'msg_emb_18', 'msg_emb_19', 'msg_emb_20', 'msg_emb_21', 'msg_emb_22', 'msg_emb_23', 'msg_emb_24', 'msg_emb_25', 'msg_emb_26', 'msg_emb_27', 'msg_emb_28', 'msg_emb_29', 'msg_emb_30', 'msg_emb_31', 'msg_emb_32', 'msg_emb_33', 'msg_emb_34', 'msg_emb_35', 'msg_emb_36', 'msg_emb_37', 'msg_emb_38', 'msg_emb_39', 'msg_emb_40', 'msg_emb_41', 'msg_emb_42', 'msg_emb_43', 'msg_emb_44', 'datetime', 'commit', 'repo', 'filepath', 'content', 'methods', 'lines', 'author_email', 'canonical_datetime', 'label', 'has_fix_kw', 'has_bug_kw']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.values.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DP_Thesis-sh7Ft33M",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
