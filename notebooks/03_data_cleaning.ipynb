{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8696134",
   "metadata": {},
   "source": [
    "# 03 - Checking Data Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e193ac2f",
   "metadata": {},
   "source": [
    "## Setting Up Project Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e1c9c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyter_init import setup\n",
    "\n",
    "setup()\n",
    "\n",
    "from src_code.config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b63078",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8dafc3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataframe with 118129 rows and 31 columns\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime                      datetime64[us, pytz.FixedOffset(-120)]\n",
       "commit                                                        object\n",
       "repo                                                          object\n",
       "filepath                                                      object\n",
       "content                                                       object\n",
       "methods                                                       object\n",
       "lines                                                         object\n",
       "author_email                                                  object\n",
       "canonical_datetime                               datetime64[ns, UTC]\n",
       "author_exp_pre                                                 int64\n",
       "author_recent_activity_pre                                     int64\n",
       "label                                                          int64\n",
       "loc_added                                                      int64\n",
       "loc_deleted                                                    int64\n",
       "files_changed                                                  int64\n",
       "hunks_count                                                    int64\n",
       "msg_len                                                        int64\n",
       "has_fix_kw                                                     int64\n",
       "has_bug_kw                                                     int64\n",
       "ast_delta                                                      int64\n",
       "complexity_delta                                               int64\n",
       "max_func_change                                                int64\n",
       "time_since_last_change                                         int64\n",
       "todo                                                           int64\n",
       "fixme                                                          int64\n",
       "try                                                            int64\n",
       "except                                                         int64\n",
       "raise                                                          int64\n",
       "code_embed                                                    object\n",
       "msg_embed                                                     object\n",
       "recent_churn                                                   int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "TRANSFORMED_DF = EXTRACTED_DATA_DIR / \"train_labeled_features_partial.feather\"\n",
    "\n",
    "# ---- LOAD ----\n",
    "df = pd.read_feather(TRANSFORMED_DF)\n",
    "print(f\"Loaded dataframe with {len(df)} rows and {len(df.columns)} columns\\n\")\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6eb0e6",
   "metadata": {},
   "source": [
    "## Converting NumpyArray -> List\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43f4dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the NumPy arrays back to Python lists\n",
    "for col in ['code_embed', 'msg_embed', 'methods', 'lines']:\n",
    "    # Use .apply(list) or .apply(lambda x: x.tolist()) for robustness\n",
    "    df[col] = df[col].apply(list)\n",
    "\n",
    "# print(df['content'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8972c09",
   "metadata": {},
   "source": [
    "## Missing Value Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfa4ad42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 1. Missing Values per Column\n",
      "|                            |   0 |\n",
      "|:---------------------------|----:|\n",
      "| datetime                   |   0 |\n",
      "| commit                     |   0 |\n",
      "| repo                       |   0 |\n",
      "| filepath                   |   0 |\n",
      "| content                    |   0 |\n",
      "| methods                    |   0 |\n",
      "| lines                      |   0 |\n",
      "| author_email               |   0 |\n",
      "| canonical_datetime         |   0 |\n",
      "| author_exp_pre             |   0 |\n",
      "| author_recent_activity_pre |   0 |\n",
      "| label                      |   0 |\n",
      "| loc_added                  |   0 |\n",
      "| loc_deleted                |   0 |\n",
      "| files_changed              |   0 |\n",
      "| hunks_count                |   0 |\n",
      "| msg_len                    |   0 |\n",
      "| has_fix_kw                 |   0 |\n",
      "| has_bug_kw                 |   0 |\n",
      "| ast_delta                  |   0 |\n",
      "| complexity_delta           |   0 |\n",
      "| max_func_change            |   0 |\n",
      "| time_since_last_change     |   0 |\n",
      "| todo                       |   0 |\n",
      "| fixme                      |   0 |\n",
      "| try                        |   0 |\n",
      "| except                     |   0 |\n",
      "| raise                      |   0 |\n",
      "| code_embed                 |   0 |\n",
      "| msg_embed                  |   0 |\n",
      "| recent_churn               |   0 |\n"
     ]
    }
   ],
   "source": [
    "print(\"## 1. Missing Values per Column\")\n",
    "nulls = df.isnull().sum().sort_values(ascending=False)\n",
    "print(nulls.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d9e187",
   "metadata": {},
   "source": [
    "## Primary Key Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "449493f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 2. Primary Key Uniqueness Check\n",
      "Duplicate key rows: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"## 2. Primary Key Uniqueness Check\")\n",
    "key_cols = [\"repo\", \"commit\", \"filepath\"]\n",
    "\n",
    "dupes = df.duplicated(subset=key_cols).sum()\n",
    "print(f\"Duplicate key rows: {dupes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c918fc9",
   "metadata": {},
   "source": [
    "## Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bb5e4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 3. Label Distribution\n",
      "|   label |   proportion |\n",
      "|--------:|-------------:|\n",
      "|       0 |     0.584483 |\n",
      "|       1 |     0.415517 |\n"
     ]
    }
   ],
   "source": [
    "print(\"## 3. Label Distribution\")\n",
    "print(df['label'].value_counts(normalize=True).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a438280c",
   "metadata": {},
   "source": [
    "## Repository Distribution (Imbalance Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "699093f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 4. Repository Distribution\n",
      "| repo    |   proportion |\n",
      "|:--------|-------------:|\n",
      "| ansible |   0.383859   |\n",
      "| sentry  |   0.286447   |\n",
      "| pandas  |   0.130119   |\n",
      "| core    |   0.119247   |\n",
      "| airflow |   0.071643   |\n",
      "| numpy   |   0.00868536 |\n"
     ]
    }
   ],
   "source": [
    "print(\"## 4. Repository Distribution\")\n",
    "repo_dist = df['repo'].value_counts(normalize=True)\n",
    "print(repo_dist.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6fa3c1",
   "metadata": {},
   "source": [
    "## Value Range Scan for Numeric Columns\n",
    "\n",
    "Automatically detects:\n",
    "\n",
    "- negatives where not allowed\n",
    "- max values\n",
    "- suspicious spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f87b9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 5. Numeric Column Range Scan\n",
      "|                            |     min |   median |          mean |              max |\n",
      "|:---------------------------|--------:|---------:|--------------:|-----------------:|\n",
      "| author_exp_pre             |       0 |      152 |   699.665     |   9456           |\n",
      "| author_recent_activity_pre |       0 |       21 |    58.569     |    709           |\n",
      "| label                      |       0 |        0 |     0.415517  |      1           |\n",
      "| loc_added                  |       0 |       16 |    92.1221    |  18552           |\n",
      "| loc_deleted                |       0 |       34 |   118.311     |  26961           |\n",
      "| files_changed              |       0 |        4 |    21.522     |    501           |\n",
      "| hunks_count                |       0 |       18 |    89.2585    |   5920           |\n",
      "| msg_len                    |       1 |       76 |   175.109     |   9628           |\n",
      "| has_fix_kw                 |       0 |        0 |     0.314744  |      1           |\n",
      "| has_bug_kw                 |       0 |        0 |     0.0622137 |      1           |\n",
      "| ast_delta                  |       0 |        0 |   709.076     | 861624           |\n",
      "| complexity_delta           |       0 |        0 |    23.7683    |  29615           |\n",
      "| max_func_change            |       0 |       49 |    78.9669    |   1180           |\n",
      "| time_since_last_change     | -641148 |     1594 | 16016.7       |      3.5335e+07  |\n",
      "| todo                       |       0 |        0 |     0.158308  |    108           |\n",
      "| fixme                      |       0 |        0 |     0.0536628 |     52           |\n",
      "| try                        |       0 |        0 |    59.9135    | 251603           |\n",
      "| except                     |       0 |        0 |     6.34085   |   8444           |\n",
      "| raise                      |       0 |        0 |     1.7866    |   1855           |\n",
      "| recent_churn               |       0 |     1073 | 16322.2       |      1.97798e+06 |\n"
     ]
    }
   ],
   "source": [
    "print(\"## 5. Numeric Column Range Scan\")\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "ranges = pd.DataFrame({\n",
    "    \"min\": df[num_cols].min(),\n",
    "    \"median\": df[num_cols].median(),\n",
    "    \"mean\": df[num_cols].mean(),\n",
    "    \"max\": df[num_cols].max()\n",
    "})\n",
    "\n",
    "print(ranges.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bb1367",
   "metadata": {},
   "source": [
    "## Check Columns Expected to Be Non-Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d899461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 6. Negative Value Check\n",
      "loc_added: 0 negative values\n",
      "loc_deleted: 0 negative values\n",
      "files_changed: 0 negative values\n",
      "hunks_count: 0 negative values\n",
      "msg_len: 0 negative values\n",
      "ast_delta: 0 negative values\n",
      "complexity_delta: 0 negative values\n",
      "max_func_change: 0 negative values\n",
      "author_exp_pre: 0 negative values\n",
      "author_recent_activity_pre: 0 negative values\n",
      "todo: 0 negative values\n",
      "fixme: 0 negative values\n",
      "try: 0 negative values\n",
      "except: 0 negative values\n",
      "raise: 0 negative values\n",
      "recent_churn: 0 negative values\n"
     ]
    }
   ],
   "source": [
    "non_negative_cols = [\n",
    "    \"loc_added\", \"loc_deleted\",\n",
    "    \"files_changed\", \"hunks_count\",\n",
    "    \"msg_len\", \"ast_delta\",\n",
    "    \"complexity_delta\", \"max_func_change\",\n",
    "    \"author_exp_pre\", \"author_recent_activity_pre\",\n",
    "    \"todo\", \"fixme\", \"try\", \"except\", \"raise\",\n",
    "    \"recent_churn\"\n",
    "]\n",
    "\n",
    "print(\"## 6. Negative Value Check\")\n",
    "for col in non_negative_cols:\n",
    "    bad = (df[col] < 0).sum()\n",
    "    print(f\"{col}: {bad} negative values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f57675",
   "metadata": {},
   "source": [
    "## Suspicious Feature Check: time_since_last_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5727505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 7. time_since_last_change Outliers\n",
      "Negative values: 66\n",
      "99.9% quantile: 1021456.0460000354\n",
      "Min: -641148\n",
      "Max: 35334961\n"
     ]
    }
   ],
   "source": [
    "print(\"## 7. time_since_last_change Outliers\")\n",
    "tslc = df[\"time_since_last_change\"]\n",
    "\n",
    "print(f\"Negative values: {(tslc < 0).sum()}\")\n",
    "print(f\"99.9% quantile: {tslc.quantile(0.999)}\")\n",
    "print(f\"Min: {tslc.min()}\")\n",
    "print(f\"Max: {tslc.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac89da7b",
   "metadata": {},
   "source": [
    "### Understanding the Feature\n",
    "\n",
    "*time_since_last_change = c.committed_date - last_time*\n",
    "\n",
    "Where:\n",
    "- c.committed_date = current commit timestamp (UNIX seconds)\n",
    "- last_time = timestamp of first parent commit\n",
    "\n",
    "So the feature = time difference between consecutive commits.\n",
    "\n",
    "This represents how much time passed between commits in a repo.\n",
    "\n",
    "### Why Negative Values?\n",
    "\n",
    "Meaning:\n",
    "\n",
    "- Some commits appear to be ~4.6 days negative (-396,818 sec)\n",
    "- Some commits appear to be ~77 days ahead (6.6M sec)\n",
    "\n",
    "This is expected when real Git data is used.\n",
    "\n",
    "Git timestamps can go backwards because:\n",
    "\n",
    "#### (1) Rebased or rewritten history\n",
    "\n",
    "During rebases, old commits appear “later” than newer ones.\n",
    "\n",
    "*WHY?*\n",
    "\n",
    "When you merge/rebase, Git does not reorder commits chronologically.\n",
    "Instead, it preserves the logical order of development.\n",
    "\n",
    "#### (2) Merge parents\n",
    "\n",
    "You use only the first parent:\n",
    "\n",
    "if c.parents:\n",
    "    last_time = c.parents[0].committed_date\n",
    "\n",
    "\n",
    "But merges may introduce non-linear time ordering.\n",
    "\n",
    "#### (3) Clock drift\n",
    "\n",
    "Different authors → different local machine clocks.\n",
    "\n",
    "#### (4) Shallow clones or incomplete history\n",
    "\n",
    "If the repo is shallow-fetched, parent commits may have weird timestamps.\n",
    "\n",
    "None of this indicates your extraction is wrong.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad362de1",
   "metadata": {},
   "source": [
    "## Binary Flag Integity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaf20288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 8. Binary Columns Integrity\n",
      "has_fix_kw: 0 invalid values\n",
      "has_bug_kw: 0 invalid values\n"
     ]
    }
   ],
   "source": [
    "print(\"## 8. Binary Columns Integrity\")\n",
    "bin_cols = [\"has_fix_kw\", \"has_bug_kw\"]\n",
    "\n",
    "for col in bin_cols:\n",
    "    bad = df[~df[col].isin([0,1])]\n",
    "    print(f\"{col}: {len(bad)} invalid values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abf7f3a",
   "metadata": {},
   "source": [
    "## Embedding Consistency Check\n",
    "\n",
    "Ensure:\n",
    "\n",
    "- no None\n",
    "- all lists\n",
    "- identical dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36efd21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "## 9. Embedding Structural Checks\n",
      "code_embed None count: 0\n",
      "msg_embed None count: 0\n",
      "\n",
      "Non-list code_embed rows: 0\n",
      "Non-list msg_embed rows: 0\n",
      "\n",
      "Embedding dimensionality distribution:\n",
      "code_embed\n",
      "768    111567\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# # Convert the NumPy arrays back to Python lists\n",
    "# for col in ['code_embed', 'msg_embed']:\n",
    "#     # Use .apply(list) or .apply(lambda x: x.tolist()) for robustness\n",
    "#     df[col] = df[col].apply(list)\n",
    "\n",
    "print(type(df.loc[0, 'code_embed']))\n",
    "print(type(df.loc[0, 'msg_embed']))\n",
    "\n",
    "print(\"## 9. Embedding Structural Checks\")\n",
    "\n",
    "# None count\n",
    "print(\"code_embed None count:\", df['code_embed'].isna().sum())\n",
    "print(\"msg_embed None count:\", df['msg_embed'].isna().sum())\n",
    "\n",
    "# Check if all are lists\n",
    "print(\"\\nNon-list code_embed rows:\", (~df['code_embed'].apply(lambda x: isinstance(x, list))).sum())\n",
    "print(\"Non-list msg_embed rows:\", (~df['msg_embed'].apply(lambda x: isinstance(x, list))).sum())\n",
    "\n",
    "# Check dimensionality\n",
    "dims = df['code_embed'].apply(lambda x: len(x) if isinstance(x, list) else None)\n",
    "print(\"\\nEmbedding dimensionality distribution:\")\n",
    "print(dims.value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5372970",
   "metadata": {},
   "source": [
    "## Datetime Consistency\n",
    "\n",
    "Check for:\n",
    "\n",
    "- NaT values\n",
    "- ordering sanity (commit should not be older than file's previous record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "752e0559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 10. Datetime Columns Audit\n",
      "datetime: NaT count = 0\n",
      "datetime: min = 2005-09-20 01:00:26-02:00, max = 2022-01-03 09:36:56-02:00\n",
      "canonical_datetime: NaT count = 0\n",
      "canonical_datetime: min = 2005-09-20 03:00:26+00:00, max = 2022-01-04 00:35:26+00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"## 10. Datetime Columns Audit\")\n",
    "\n",
    "date_cols = [\"datetime\", \"canonical_datetime\"]\n",
    "\n",
    "for col in date_cols:\n",
    "    print(f\"{col}: NaT count = {df[col].isna().sum()}\")\n",
    "    print(f\"{col}: min = {df[col].min()}, max = {df[col].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76ca36d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 12. Columns With Only One Unique Value\n"
     ]
    }
   ],
   "source": [
    "print(\"## 12. Columns With Only One Unique Value\")\n",
    "\n",
    "for col in df.columns:\n",
    "    # Convert arrays to tuples for uniqueness check\n",
    "    if df[col].apply(lambda x: isinstance(x, (np.ndarray, list))).any():\n",
    "        # Convert each element to a tuple (or a string representation)\n",
    "        # print(col)\n",
    "        unique_values = df[col].apply(lambda x: tuple(x) if isinstance(x, (np.ndarray, list)) else x)\n",
    "    else:\n",
    "        unique_values = df[col]\n",
    "\n",
    "    if unique_values.nunique(dropna=True) == 1:\n",
    "        print(f\"⚠️ {col} has only one unique value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ef3f8",
   "metadata": {},
   "source": [
    "## Check Text Columns for Weirdness\n",
    "\n",
    "Empty strings? Too short? Too long?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6e31166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 13. Text Field Checks\n",
      "Empty content rows: 2406\n",
      "count    111567.000000\n",
      "mean       1840.187071\n",
      "std        4971.425020\n",
      "min           0.000000\n",
      "25%         403.000000\n",
      "50%         759.000000\n",
      "75%        1700.000000\n",
      "max      227258.000000\n",
      "Name: content, dtype: float64\n",
      "\n",
      "Empty methods rows: 33930\n",
      "count    111567.000000\n",
      "mean          1.849185\n",
      "std           3.565718\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           1.000000\n",
      "75%           2.000000\n",
      "max         148.000000\n",
      "Name: methods, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"## 13. Text Field Checks\")\n",
    "\n",
    "if 'content' in df.columns:\n",
    "    print(\"Empty content rows:\", (df['content'].str.len() == 0).sum())\n",
    "    print(df['content'].str.len().describe())\n",
    "\n",
    "# Check 'methods' if it is a list column\n",
    "if 'methods' in df.columns:\n",
    "    # Use len() on the Python lists\n",
    "    print(\"\\nEmpty methods rows:\", (df['methods'].apply(len) == 0).sum())\n",
    "    print(df['methods'].apply(len).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934954c2",
   "metadata": {},
   "source": [
    "## Check For Impossible Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f17bad1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 14. Logical Consistency Checks\n",
      "msg_len outliers (msg_len <= 0): 0\n"
     ]
    }
   ],
   "source": [
    "print(\"## 14. Logical Consistency Checks\")\n",
    "\n",
    "# msg_len should match commit message length\n",
    "if \"msg_len\" in df.columns:\n",
    "    print(\"msg_len outliers (msg_len <= 0):\", (df['msg_len'] <= 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72432152",
   "metadata": {},
   "source": [
    "## Filepath Sanity\n",
    "Check for Windows vs POSIX weirdness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6652381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filepaths without / (unexpected after normalization): 1275\n",
      "11        setup.py\n",
      "80        setup.py\n",
      "81     setupegg.py\n",
      "122       setup.py\n",
      "123       setup.py\n",
      "138       setup.py\n",
      "144       setup.py\n",
      "168       setup.py\n",
      "278       setup.py\n",
      "461       setup.py\n",
      "462       setup.py\n",
      "479       setup.py\n",
      "497       setup.py\n",
      "513       setup.py\n",
      "514       setup.py\n",
      "523       setup.py\n",
      "524       setup.py\n",
      "531       setup.py\n",
      "562       setup.py\n",
      "570       setup.py\n",
      "Name: filepath, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Normalize all filepaths to use forward slashes\n",
    "df['filepath'] = df['filepath'].str.replace('\\\\', '/', regex=False)\n",
    "\n",
    "# Check again for paths without a slash\n",
    "bad_paths = df[~df['filepath'].str.contains('/')]\n",
    "print(\"Filepaths without / (unexpected after normalization):\", len(bad_paths))\n",
    "print(df.loc[~df['filepath'].str.contains('/'), 'filepath'].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e72a7b",
   "metadata": {},
   "source": [
    "## Check Recent Churn for Extreme Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3fd5421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 16. recent_churn Outlier Scan\n",
      "count    1.115670e+05\n",
      "mean     1.632216e+04\n",
      "std      9.333987e+04\n",
      "min      0.000000e+00\n",
      "25%      7.200000e+01\n",
      "50%      1.073000e+03\n",
      "75%      6.104000e+03\n",
      "max      1.977985e+06\n",
      "Name: recent_churn, dtype: float64\n",
      "99.9% quantile: 1384887.4380000094\n"
     ]
    }
   ],
   "source": [
    "print(\"## 16. recent_churn Outlier Scan\")\n",
    "print(df['recent_churn'].describe())\n",
    "print(\"99.9% quantile:\", df['recent_churn'].quantile(0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbf4792",
   "metadata": {},
   "source": [
    "## Check Distribution of Code Activity Keywords\n",
    "\n",
    "(todo, fixme, try/except/raise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa24f007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 17. Keyword Column Distributions\n",
      "|        |   count |       mean |        std |   min |   25% |   50% |   75% |    max |\n",
      "|:-------|--------:|-----------:|-----------:|------:|------:|------:|------:|-------:|\n",
      "| todo   |  111567 |  0.158308  |    2.68773 |     0 |     0 |     0 |     0 |    108 |\n",
      "| fixme  |  111567 |  0.0536628 |    1.13933 |     0 |     0 |     0 |     0 |     52 |\n",
      "| try    |  111567 | 59.9135    | 3540.12    |     0 |     0 |     0 |     1 | 251603 |\n",
      "| except |  111567 |  6.34085   |  205.617   |     0 |     0 |     0 |     0 |   8444 |\n",
      "| raise  |  111567 |  1.7866    |   45.3334  |     0 |     0 |     0 |     0 |   1855 |\n"
     ]
    }
   ],
   "source": [
    "print(\"## 17. Keyword Column Distributions\")\n",
    "kw_cols = [\"todo\", \"fixme\", \"try\", \"except\", \"raise\"]\n",
    "\n",
    "print(df[kw_cols].describe().T.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07619a53",
   "metadata": {},
   "source": [
    "## Duplicate commit-message / code-embed lengths\n",
    "\n",
    "Check uniformity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a880db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code_embed\n",
      "768    111567\n",
      "Name: count, dtype: int64\n",
      "msg_embed\n",
      "768    111567\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "code_duplicates = df['code_embed'].apply(len).value_counts().head()\n",
    "msg_duplicates = df['msg_embed'].apply(len).value_counts().head()\n",
    "\n",
    "print(code_duplicates)\n",
    "print(msg_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8f4a7a",
   "metadata": {},
   "source": [
    "So all your embeddings being length 768 means:\n",
    "\n",
    "- The model you are using outputs a 768-dimensional vector\n",
    "- The embedding extraction process worked for every commit\n",
    "- No corrupted or empty embeddings\n",
    "- No input missing (no None, no NaN, no empty list, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa72d1",
   "metadata": {},
   "source": [
    "## File extension distribution\n",
    "This reveals if:\n",
    "\n",
    "- non-Python files contaminate AST/keyword metrics\n",
    "- certain extensions dominate bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "349f52b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ext\n",
       "py     111452\n",
       "pyi       115\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ext'] = df['filepath'].str.split('.').str[-1]\n",
    "df['ext'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b506f",
   "metadata": {},
   "source": [
    "This does not mean the entire Pandas repo only contains .py files.\n",
    "It means:\n",
    "\n",
    "All diffs included in your dataset modify .py or .pyi files.\n",
    "\n",
    "The Defectors dataset:\n",
    "\n",
    "- extracts commits that add/remove lines regarded as “faulty”\n",
    "- focuses on logical/code changes, not text, docs, or build files\n",
    "- filters out non-source changes for consistency\n",
    "\n",
    "So your dataset is a filtered view of the repo, not the repo itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67ca73a",
   "metadata": {},
   "source": [
    "## Diff size sanity\n",
    "\n",
    "Large mismatches could indicate:\n",
    "\n",
    "- truncated diffs\n",
    "- non-standard diff formatting\n",
    "- metadata lines (prefix +++, ---, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b168c242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    111567.000000\n",
       "mean       -165.518289\n",
       "std         572.416860\n",
       "min      -30859.000000\n",
       "25%        -168.000000\n",
       "50%         -24.000000\n",
       "75%           7.000000\n",
       "max        5359.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df[\"content\"].str.count(\"\\n\") - df[\"loc_added\"] - df[\"loc_deleted\"]).describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02708ab2",
   "metadata": {},
   "source": [
    "The discrepancy between content line counts and loc_added + loc_deleted is normal in real Git data.\n",
    "\n",
    "Reasons:\n",
    "\n",
    "1. Multi-line statements / code folding in diffs\n",
    "2. Partial line changes counted in hunks\n",
    "3. Files with removed lines only (content now shorter)\n",
    "4. Large diffs in a single commit skew the stats\n",
    "\n",
    "No indication of extraction errors — these are just properties of real commit diffs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src_code-O5IEtaYk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
