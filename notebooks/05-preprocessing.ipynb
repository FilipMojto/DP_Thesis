{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262d8bc5",
   "metadata": {},
   "source": [
    "# 05 - Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a9bacf",
   "metadata": {},
   "source": [
    "## 5.1 - Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63351fa",
   "metadata": {},
   "source": [
    "### 5.1.1 - Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4ebf446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging_config import NotebookLogger\n",
    "from constants import LOG_FILE\n",
    "logger = NotebookLogger(label=\"PREPROCESSING\", notebook_name=None, file_log_path=LOG_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39586ea7",
   "metadata": {},
   "source": [
    "### 5.1.2 Configuring Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7596c094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PREPROCESSING CHECK] Setting up root by appending the parent to the sys...\n",
      "[PREPROCESSING RESULT] Done.\n"
     ]
    }
   ],
   "source": [
    "from jupyter_init import setup\n",
    "\n",
    "logger.log_check(\"Setting up root by appending the parent to the sys...\", print_to_console=True)\n",
    "\n",
    "setup()\n",
    "\n",
    "logger.log_result(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12db75e",
   "metadata": {},
   "source": [
    "### 5.1.3 Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1286f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# System & External Libs\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# My Libs\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from src_code.config import FITTED_TRANSFORMER\n",
    "from src_code.ml_pipeline.preprocessing.transformers import transform\n",
    "from src_code.config import *\n",
    "from src_code.ml_pipeline.df_load import load_df\n",
    "from src_code.ml_pipeline.preprocessing.transformers import pca_explained_variance\n",
    "from src_code.ml_pipeline.df_load import save_df\n",
    "from src_code.ml_pipeline.preprocessing.preprocessing import drop_invalid_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5106823f",
   "metadata": {},
   "source": [
    "### 5.1.4 Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a551fc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PREPROCESSING CHECK] Loading the dataset...\n",
      "C:\\Users\\fmojt\\Code\\DPThesis\\DP_Thesis\\data\\interim\\test_labeled_features_partial_v10.feather\n",
      "[PREPROCESSING CHECK] Loading the dataset...\n",
      "[PREPROCESSING RESULT] Loaded dataframe with 7363 rows and 31 columns\n",
      "\n",
      "[PREPROCESSING RESULT] Dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "logger.log_check(\"Loading the dataset...\", print_to_console=True)\n",
    "\n",
    "# TARGET_DF_FILE = ETL_MAPPINGS['test']['current_newest']\n",
    "subset: SubsetType = 'test'\n",
    "TARGET_DF_FILE = PREPROCESSING_MAPPINGS[subset]['input']\n",
    "print(ETL_MAPPINGS[subset]['current_newest'])\n",
    "\n",
    "# ---- LOAD ----\n",
    "df = load_df(df_file_path=TARGET_DF_FILE, logger=logger)\n",
    "# df = pd.read_feather(TARGET_DF_FILE)\n",
    "# log_result(f\"Loaded dataframe with {len(df)} rows and {len(df.columns)} columns\\n\", print_to_console=True)\n",
    "\n",
    "# # For large datasets\n",
    "# pd.set_option('display.max_columns', 50)\n",
    "# sns.set_theme(style=\"whitegrid\", context=\"notebook\", palette=\"muted\")\n",
    "logger.log_result(\"Dataset loaded successfully.\", print_to_console=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9963d811",
   "metadata": {},
   "source": [
    "## 5.2 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c15062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# preprocessor = ColumnTransformer(transformers=[])\n",
    "# append a transformer tuple (name, transformer, columns)\n",
    "# preprocessor.transformers.append(('new_passthrough', 'passthrough', ['col1', 'col2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b4a61",
   "metadata": {},
   "source": [
    "### 5.2.1 Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c477e8eb",
   "metadata": {},
   "source": [
    "#### 5.2.1.1 - Fix negative values before log transform\n",
    "Some features (e.g., time_since_last_change) contain negative values.\n",
    "\n",
    "We shift them to be ≥ 0 before applying log1p:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d73d379f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PREPROCESSING CHECK] Applying row-level filters on numeric features...\n",
      "[PREPROCESSING RESULT] Dropping 0 rows due to filter on 'time_since_last_change'\n"
     ]
    }
   ],
   "source": [
    "# def shift_min_to_zero(df, col):\n",
    "#     \"\"\"Shift column so minimum is 0 if negative values exist.\"\"\"\n",
    "#     min_val = df[col].min()\n",
    "#     if min_val < 0:\n",
    "#         df[col] = df[col] - min_val\n",
    "#     return df\n",
    "\n",
    "# for col in NUMERIC_FEATURES:\n",
    "#     df = shift_min_to_zero(df, col)\n",
    "\n",
    "# NEG_FEATURES_TO_DROP = [\"time_since_last_change\"]\n",
    "\n",
    "# # List of features to check: NUMERIC_FEATURES excluding NEG_FEATURES_TO_DROP\n",
    "# features_to_check = [col for col in NUMERIC_FEATURES if col not in NEG_FEATURES_TO_DROP]\n",
    "\n",
    "df = drop_invalid_rows(\n",
    "    df=df,\n",
    "    # numeric_features=NUMERIC_FEATURES,\n",
    "    row_filters={\"time_since_last_change\": lambda s: s >= 0},\n",
    "    logger=logger,\n",
    "    sanity_check=True\n",
    ")\n",
    "\n",
    "# # Check if any of the features in features_to_check contain negative values\n",
    "# if any(contains_negative(df, col) for col in features_to_check):\n",
    "#     # If True, raise an exception\n",
    "#     raise ValueError(\"Unexpected negative values found in one or more numeric features that are NOT set to be dropped.\")\n",
    "\n",
    "\n",
    "# neg_mask = df[\"time_since_last_change\"] < 0\n",
    "# n_neg = neg_mask.sum()\n",
    "\n",
    "# print(f\"Dropping {n_neg} rows with negative time_since_last_change\")\n",
    "\n",
    "# df = df[~neg_mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219ac218",
   "metadata": {},
   "source": [
    "#### 5.2.1.2 Assertion Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38b75e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_check(\"[NEG_FEATURES_TO_DROP] Performing assertion check...\")\n",
    "# assert(any(contains_negative(df, col) for col in NEG_FEATURES_TO_DROP) == False)\n",
    "# log_result(\"[NEG_FEATURES_TO_DROP] Check succesfull!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563eeb35",
   "metadata": {},
   "source": [
    "### 5.2.2 Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03ae082",
   "metadata": {},
   "source": [
    "#### 5.2.2.1 - Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85379de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c8f4ea",
   "metadata": {},
   "source": [
    "#### 5.2.2.2 - Transformations\n",
    "\n",
    "Applies the same bounds your EDA used.\n",
    "\n",
    "You want preprocessing to match your EDA findings, so we clamp values to the lower/upper fences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c73e8a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def expand_embedding(df, col_name, prefix):\n",
    "#     # Converts a column of arrays into a matrix\n",
    "#     emb = np.vstack(df[col_name].values)\n",
    "#     emb_df = pd.DataFrame(\n",
    "#         emb,\n",
    "#         index=df.index,\n",
    "#         columns=[f\"{prefix}_{i}\" for i in range(emb.shape[1])]\n",
    "#     )\n",
    "#     return emb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b487378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PREPROCESSING CHECK] Performing df transformation...\n",
      "[PREPROCESSING RESULT] Detected test subset. Loading fitted preprocessor...\n",
      "[PREPROCESSING RESULT] Transformations applied successfully.\n"
     ]
    }
   ],
   "source": [
    "df, transformer = transform(df=df,\n",
    "        subset=subset,\n",
    "          logger=logger,\n",
    "          random_state=RANDOM_STATE,\n",
    "          fitted_transformer=FITTED_TRANSFORMER)\n",
    "\n",
    "# fitted_transfomer: ColumnTransformer = joblib.load(FITTED_TRANSFORMER)\n",
    "\n",
    "\n",
    "# set_config(transform_output='pandas')\n",
    "# log_transformer = FunctionTransformer(np.log1p, validate=False)\n",
    "\n",
    "# if subset == 'train':\n",
    "#     # log_check(\"Detected train subset. Creating new preprocessor...\", print_to_console=True)\n",
    "#     # preprocessor = ColumnTransformer(transformers=[], remainder='passthrough', verbose_feature_names_out=False)\n",
    "\n",
    "#     # preprocessor.transformers.append(('winsorize', WinsorizerIQR(factor=1.5), NUMERIC_FEATURES))\n",
    "#     # preprocessor.transformers.append(('log_tokens', log_transformer, LINE_TOKEN_FEATURES))\n",
    "#     # preprocessor.transformers.append(('log_numeric', log_transformer, NUMERIC_FEATURES))\n",
    "\n",
    "#     # # 3. FIT the preprocessor ONLY on the training data\n",
    "#     # preprocessor.fit(df)\n",
    "#     # df = preprocessor.transform(df)\n",
    "\n",
    "#     # # 4. SAVE the fitted preprocessor\n",
    "#     # # The saved object contains all the calculated Q1, Q3 bounds.\n",
    "#     # joblib.dump(preprocessor, FITTED_PREPROCESSOR)\n",
    "#     log_check(\"Detected train subset. Creating new preprocessor...\", print_to_console=True)\n",
    "\n",
    "#     # code_emb_df = expand_embedding(df, \"code_embed\", \"code_emb\")\n",
    "#     # msg_emb_df  = expand_embedding(df, \"msg_embed\", \"msg_emb\")\n",
    "#     # df = pd.concat([df.drop(columns=[\"code_embed\", \"msg_embed\"]), code_emb_df, msg_emb_df], axis=1)\n",
    "\n",
    "#     # Update the EMBEDDINGS constant to reflect the NEW flattened column names\n",
    "#     # FLATTENED_EMBEDDINGS = code_emb_df.columns.tolist() + msg_emb_df.columns.tolist()\n",
    "\n",
    "#     # Define a pipeline for EACH embedding type\n",
    "#     # code_emb_pipe = Pipeline([\n",
    "#     #     ('expand', EmbeddingExpander(prefix=\"code_emb\")),\n",
    "#     #     ('pca', PCA(n_components=100, random_state=RANDOM_STATE))\n",
    "#     # ])\n",
    "#     # Use it in your pipeline like this:\n",
    "#     code_emb_pipe = Pipeline([\n",
    "#         ('expand', EmbeddingExpander(prefix=\"code\")),\n",
    "#         ('pca', NamingPCA(n_components=10, prefix=\"code_emb_\", random_state=RANDOM_STATE))\n",
    "#     ])\n",
    "\n",
    "#     msg_emb_pipe = Pipeline([\n",
    "#         ('expand', EmbeddingExpander(prefix=\"msg\")),\n",
    "#         # ('pca', PCA(n_components=100, random_state=RANDOM_STATE))\n",
    "#         ('pca', NamingPCA(n_components=45, prefix=\"msg_emb_\", random_state=RANDOM_STATE))\n",
    "\n",
    "#     ])\n",
    "\n",
    "#     # 1. Define a pipeline for numeric features: Winsorize THEN Log\n",
    "#     numeric_pipeline = Pipeline([\n",
    "#         ('winsorize', WinsorizerIQR(factor=1.5)),\n",
    "#         ('log', log_transformer),\n",
    "#         (\"var_thresh\", VarianceThreshold(threshold=0.0))\n",
    "#     ])\n",
    "\n",
    "#     # embedding_transformer = Pipeline(steps=[\n",
    "#     #     (\"pca\", PCA(n_components=100, random_state=RANDOM_STATE))\n",
    "#     # ])\n",
    "\n",
    "\n",
    "#     # 2. Setup the ColumnTransformer\n",
    "#     preprocessor = ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             # ('num_transformed', numeric_pipeline, NUMERIC_FEATURES),\n",
    "#             # ('token_transformed', log_transformer, LINE_TOKEN_FEATURES),\n",
    "#             # (\"embed\", embedding_transformer, FLATTENED_EMBEDDINGS),\n",
    "#             ('num', numeric_pipeline, NUMERIC_FEATURES),\n",
    "#             ('tokens', log_transformer, LINE_TOKEN_FEATURES),\n",
    "#             ('code_embed', code_emb_pipe, ['code_embed']), # Pass as list\n",
    "#             ('msg_embed', msg_emb_pipe, ['msg_embed']),    # Pass as list\n",
    "#         ],\n",
    "#         remainder='passthrough',\n",
    "#         verbose_feature_names_out=False  # This now works because names are unique\n",
    "#     )\n",
    "\n",
    "#     # 3. FIT and TRANSFORM\n",
    "#     preprocessor.fit(df)\n",
    "#     df = preprocessor.transform(df)\n",
    "\n",
    "#     # 4. SAVE\n",
    "#     joblib.dump(preprocessor, FITTED_TRANSFORMER)\n",
    "\n",
    "#     # print(\"Fitted preprocessor saved to fitted_preprocessor.joblib\")\n",
    "# elif subset in ('test', 'validate'):\n",
    "#     log_check(\"Detected test subset. Loading fitted preprocessor...\", print_to_console=True)\n",
    "#     loaded_preprocessor = joblib.load(FITTED_TRANSFORMER)\n",
    "#     df = loaded_preprocessor.transform(df)\n",
    "# else:\n",
    "#     msg = \"Unknown subset value!\"\n",
    "#     logger.error(msg)\n",
    "#     raise ValueError(msg)\n",
    "\n",
    "\n",
    "# log_result(\"Transformations applied successfully.\", print_to_console=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# numeric_transformer = Pipeline(steps=[\n",
    "#     (\"var_thresh\", VarianceThreshold(threshold=0.0))\n",
    "# ])\n",
    "\n",
    "# embedding_transformer = Pipeline(steps=[\n",
    "#     (\"pca\", PCA(n_components=100, random_state=RANDOM_STATE))\n",
    "# ])\n",
    "\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         # (\"struct\", \"passthrough\", structured_features),\n",
    "#         (\"struct\", numeric_transformer, structured_features),\n",
    "#         (\"embed\", embedding_transformer, embedding_features),\n",
    "#     ],\n",
    "#     remainder=\"drop\"\n",
    "# )\n",
    "\n",
    "# def winsorize_iqr(df, col, preserve_original: bool = False):\n",
    "#     \"\"\"\n",
    "#     Caps extreme outliers using IQR fences.\n",
    "#     Keeps the distribution shape mostly intact.\n",
    "#     \"\"\"\n",
    "#     Q1 = df[col].quantile(0.25)\n",
    "#     Q3 = df[col].quantile(0.75)\n",
    "#     IQR = Q3 - Q1\n",
    "#     lower = Q1 - 1.5 * IQR\n",
    "#     upper = Q3 + 1.5 * IQR\n",
    "\n",
    "#     print(f\"Df len before winsorization ({col}): {len(df)}\")\n",
    "\n",
    "#     new_col_name = col + \"_winsorized\" if preserve_original else col\n",
    "\n",
    "#     df[new_col_name] = df[col].clip(lower=lower, upper=upper)\n",
    "#     print(f\"Df len before winsorization ({col}): {len(df)}\")\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "# --- Apply to all numeric columns ---\n",
    "# for col in NUMERIC_FEATURES:\n",
    "#     df = winsorize_iqr(df, col, preserve_original=True) if col == 'recent_churn' else winsorize_iqr(df, col)\n",
    "\n",
    "# for col in LINE_TOKEN_FEATURES:\n",
    "#     # df = winsorize_iqr(df, col)\n",
    "#     df[col] = np.log1p(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b30dbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PREPROCESSING CHECK] Checking explanation of variance by embeddings...\n",
      "[PREPROCESSING RESULT] Code embeddings explain 85.40% of variance\n",
      "[PREPROCESSING RESULT] Message embeddings explain 82.61% of variance\n"
     ]
    }
   ],
   "source": [
    "# Access the PCA step from your fitted preprocessor\n",
    "# Assuming the step was named 'code_pca' in the ColumnTransformer\n",
    "# pca_model = fitted_transfomer.named_transformers_['code_embed'].named_steps['pca']\n",
    "# total_variance = sum(pca_model.explained_variance_ratio_)\n",
    "\n",
    "# print(f\"Your 50 components explain {total_variance:.2%} of the original code data.\")\n",
    "\n",
    "# pca_model = fitted_transfomer.named_transformers_['msg_embed'].named_steps['pca']\n",
    "# total_variance = sum(pca_model.explained_variance_ratio_)\n",
    "# print(f\"Your 50 components explain {total_variance:.2%} of the original msg data.\")\n",
    "\n",
    "# from src_code.ml_pipeline.preprocessing.transformers import pca_explained_variance\n",
    "\n",
    "logger.log_check(\"Checking explanation of variance by embeddings...\")\n",
    "\n",
    "\n",
    "logger.log_result(\n",
    "    f\"Code embeddings explain \"\n",
    "    f\"{pca_explained_variance(transformer=transformer, name='code_embed'):.2%} of variance\"\n",
    ")\n",
    "\n",
    "logger.log_result(\n",
    "    f\"Message embeddings explain \"\n",
    "    f\"{pca_explained_variance(transformer=transformer, name='msg_embed'):.2%} of variance\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e4cf37",
   "metadata": {},
   "source": [
    "<!-- ### 5.2.2 - Fix negative values before log transform\n",
    "Some features (e.g., time_since_last_change) contain negative values.\n",
    "\n",
    "We shift them to be ≥ 0 before applying log1p: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0151761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def shift_min_to_zero(df, col):\n",
    "# #     \"\"\"Shift column so minimum is 0 if negative values exist.\"\"\"\n",
    "# #     min_val = df[col].min()\n",
    "# #     if min_val < 0:\n",
    "# #         df[col] = df[col] - min_val\n",
    "# #     return df\n",
    "\n",
    "# # for col in NUMERIC_FEATURES:\n",
    "# #     df = shift_min_to_zero(df, col)\n",
    "\n",
    "# from notebooks.utils import contains_negative\n",
    "# from notebooks.constants import NUMERIC_FEATURES\n",
    "\n",
    "# NEG_FEATURES_TO_DROP = ['time_since_last_change']\n",
    "\n",
    "# # List of features to check: NUMERIC_FEATURES excluding NEG_FEATURES_TO_DROP\n",
    "# features_to_check = [\n",
    "#     col for col in NUMERIC_FEATURES \n",
    "#     if col not in NEG_FEATURES_TO_DROP\n",
    "# ]\n",
    "\n",
    "# # Check if any of the features in features_to_check contain negative values\n",
    "# if any(contains_negative(df, col) for col in features_to_check):\n",
    "#     # If True, raise an exception\n",
    "#     raise ValueError(\"Unexpected negative values found in one or more numeric features that are NOT set to be dropped.\")\n",
    "\n",
    "\n",
    "# neg_mask = df[\"time_since_last_change\"] < 0\n",
    "# n_neg = neg_mask.sum()\n",
    "\n",
    "# print(f\"Dropping {n_neg} rows with negative time_since_last_change\")\n",
    "\n",
    "# df = df[~neg_mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ec505e",
   "metadata": {},
   "source": [
    "### 5.2.3 - Log1p Transformation\n",
    "Reduces heavy right-skew (your EDA showed skews up to 100+)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6892df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in NUMERIC_FEATURES:\n",
    "#     df[col] = np.log1p(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8a70b5",
   "metadata": {},
   "source": [
    "## 5.3. - Save preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7bbcbbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PREPROCESSING CHECK] Saving the preprocessed dataset...\n",
      "[PREPROCESSING RESULT] Preprocessed dataset saved to C:\\Users\\fmojt\\Code\\DPThesis\\DP_Thesis\\data\\processed\\test_preprocessed.feather\n"
     ]
    }
   ],
   "source": [
    "# log_check(\"Saving the preprocessed dataset...\", print_to_console=True)\n",
    "\n",
    "# OUTPUT_PATH = PREPROCESSING_MAPPINGS[subset]['output']\n",
    "\n",
    "# # 1. Get the names of the final features\n",
    "# # feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# # 2. Reconstruct the DataFrame\n",
    "# # df_transformed = pd.DataFrame(df, columns=feature_names)\n",
    "\n",
    "# df.to_feather(OUTPUT_PATH)\n",
    "\n",
    "# log_result(f\"Preprocessed dataset saved to {OUTPUT_PATH}\", print_to_console=True)\n",
    "\n",
    "save_df(df=df, df_file_path=PREPROCESSING_MAPPINGS[subset][\"output\"], logger=logger)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DP_Thesis-sh7Ft33M",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
