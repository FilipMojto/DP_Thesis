{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c733e47b",
   "metadata": {},
   "source": [
    "# 01. Preprocessing - Data Format Benchmark Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4bcc0f",
   "metadata": {},
   "source": [
    "## Dependencies & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5d6e544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIG] Config file directory: C:\\Users\\fmojt\\Code\\Software Projects\\DiplomaThesis\\src_code\\config.py\n",
      "[CONFIG] Project root directory: C:\\Users\\fmojt\\Code\\Software Projects\\DiplomaThesis\n",
      "[CONFIG] Data directory: C:\\Users\\fmojt\\Code\\Software Projects\\DiplomaThesis\\data\n",
      "[CONFIG] Bug inducing commits directory: C:\\Users\\fmojt\\Code\\Software Projects\\DiplomaThesis\\data\\bug_inducing_commits\n",
      "Index(['datetime', 'commit', 'repo', 'filepath', 'content', 'methods',\n",
      "       'lines'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pyarrow.feather as feather\n",
    "import os\n",
    "\n",
    "from jupyter_init import setup\n",
    "setup()\n",
    "\n",
    "from src_code.config import DEFECTORS_DIR\n",
    "\n",
    "RAW_DATASET_FILE = DEFECTORS_DIR / \"line_bug_prediction_splits/random/train.parquet.gzip\"\n",
    "\n",
    "# i need to find the first feather file where the raw dataset is stored\n",
    "FTH_DATASET_FILE = RAW_DATASET_FILE.with_suffix(\"\").with_suffix(\".feather\")\n",
    "\n",
    "DATASET_DIR = RAW_DATASET_FILE.parent\n",
    "OUTPUT_DIR = DATASET_DIR / \"bench_formats\"\n",
    "\n",
    "# load the raw dataset\n",
    "df = pd.read_parquet(RAW_DATASET_FILE)\n",
    "print(df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723eeee3",
   "metadata": {},
   "source": [
    "## Parquet -> Feather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b23d896",
   "metadata": {},
   "source": [
    "### Why Feather?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a492bd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original .parquet.gzip...\n",
      "parquet(gzip) read        1.9627 seconds   (rows=185,369)\n",
      "\n",
      "Saving to other formats...\n",
      "\n",
      "Benchmarking load speeds:\n",
      "parquet (gzip)            1.6495 seconds   (rows=185,369)\n",
      "parquet (raw)             0.8892 seconds   (rows=185,369)\n",
      "feather                   0.7575 seconds   (rows=185,369)\n",
      "pickle                    2.1212 seconds   (rows=185,369)\n"
     ]
    }
   ],
   "source": [
    "def measure_time(label, func):\n",
    "    \"\"\"Measure read time.\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    df = func()\n",
    "    end = time.perf_counter()\n",
    "    print(f\"{label:<25} {end - start:.4f} seconds   (rows={len(df):,})\")\n",
    "    return df\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Loading original .parquet.gzip...\")\n",
    "df = measure_time(\"parquet(gzip) read\", lambda: pd.read_parquet(RAW_DATASET_FILE))\n",
    "\n",
    "# Save alternative formats\n",
    "print(\"\\nSaving to other formats...\")\n",
    "\n",
    "# 1) Feather\n",
    "feather_file = OUTPUT_DIR / \"train.feather\"\n",
    "feather.write_feather(df, feather_file)\n",
    "\n",
    "# 2) Parquet uncompressed\n",
    "parquet_fast = OUTPUT_DIR / \"train_uncompressed.parquet\"\n",
    "df.to_parquet(parquet_fast, compression=None)\n",
    "\n",
    "# 3) Pickle\n",
    "pickle_file = OUTPUT_DIR / \"train.pkl\"\n",
    "df.to_pickle(pickle_file)\n",
    "\n",
    "# 4) CSV (optional — slowest & largest)\n",
    "# csv_file = OUTPUT_DIR / \"train.csv\"\n",
    "# df.to_csv(csv_file, index=False)\n",
    "\n",
    "\n",
    "print(\"\\nBenchmarking load speeds:\")\n",
    "measure_time(\"parquet (gzip)\", lambda: pd.read_parquet(RAW_DATASET_FILE))\n",
    "measure_time(\"parquet (raw)\", lambda: pd.read_parquet(parquet_fast))\n",
    "measure_time(\"feather\", lambda: feather.read_feather(feather_file))\n",
    "measure_time(\"pickle\", lambda: pd.read_pickle(pickle_file))\n",
    "# measure_time(\"csv\", lambda: pd.read_csv(csv_file))\n",
    "\n",
    "# deleting the bench formats\n",
    "for file in OUTPUT_DIR.iterdir():\n",
    "    os.remove(file)\n",
    "os.rmdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e7221",
   "metadata": {},
   "source": [
    "### Saving All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "929ab646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for .parquet/.parquet.gzip in ..\\..\\data\\defectors\n",
      "Found 12 parquet files.\n",
      "\n",
      "[LOAD]  ..\\..\\data\\defectors\\line_bug_prediction_splits\\random\\test.parquet.gzip\n",
      "[SAVE]  ..\\..\\data\\defectors\\line_bug_prediction_splits\\random\\test.feather\n",
      "→ done: load=0.900s, save=0.860s\n",
      "\n",
      "[LOAD]  ..\\..\\data\\defectors\\line_bug_prediction_splits\\random\\train.parquet.gzip\n",
      "[SAVE]  ..\\..\\data\\defectors\\line_bug_prediction_splits\\random\\train.feather\n",
      "→ done: load=22.290s, save=24.863s\n",
      "\n",
      "[LOAD]  ..\\..\\data\\defectors\\line_bug_prediction_splits\\random\\val.parquet.gzip\n",
      "[SAVE]  ..\\..\\data\\defectors\\line_bug_prediction_splits\\random\\val.feather\n",
      "→ done: load=1.158s, save=0.909s\n",
      "\n",
      "[LOAD]  ..\\..\\data\\defectors\\line_bug_prediction_splits\\time\\test.parquet.gzip\n",
      "[SAVE]  ..\\..\\data\\defectors\\line_bug_prediction_splits\\time\\test.feather\n",
      "→ done: load=0.988s, save=1.128s\n",
      "\n",
      "[LOAD]  ..\\..\\data\\defectors\\line_bug_prediction_splits\\time\\train.parquet.gzip\n",
      "[SAVE]  ..\\..\\data\\defectors\\line_bug_prediction_splits\\time\\train.feather\n",
      "→ done: load=24.100s, save=26.404s\n",
      "\n",
      "[LOAD]  ..\\..\\data\\defectors\\line_bug_prediction_splits\\time\\val.parquet.gzip\n",
      "[SAVE]  ..\\..\\data\\defectors\\line_bug_prediction_splits\\time\\val.feather\n",
      "→ done: load=1.050s, save=1.018s\n",
      "\n",
      "[LOAD]  ..\\..\\data\\defectors\\jit_bug_prediction_splits\\random\\test.parquet.gzip\n",
      "[SAVE]  ..\\..\\data\\defectors\\jit_bug_prediction_splits\\random\\test.feather\n",
      "→ done: load=0.169s, save=0.107s\n",
      "\n",
      "[LOAD]  ..\\..\\data\\defectors\\jit_bug_prediction_splits\\random\\train.parquet.gzip\n",
      "[SAVE]  ..\\..\\data\\defectors\\jit_bug_prediction_splits\\random\\train.feather\n",
      "→ done: load=1.994s, save=1.733s\n",
      "\n",
      "[LOAD]  ..\\..\\data\\defectors\\jit_bug_prediction_splits\\random\\val.parquet.gzip\n",
      "[SAVE]  ..\\..\\data\\defectors\\jit_bug_prediction_splits\\random\\val.feather\n",
      "→ done: load=0.089s, save=0.084s\n",
      "\n",
      "[LOAD]  ..\\..\\data\\defectors\\jit_bug_prediction_splits\\time\\test.parquet.gzip\n",
      "[SAVE]  ..\\..\\data\\defectors\\jit_bug_prediction_splits\\time\\test.feather\n",
      "→ done: load=0.087s, save=0.061s\n",
      "\n",
      "[LOAD]  ..\\..\\data\\defectors\\jit_bug_prediction_splits\\time\\train.parquet.gzip\n",
      "[SAVE]  ..\\..\\data\\defectors\\jit_bug_prediction_splits\\time\\train.feather\n",
      "→ done: load=2.056s, save=2.067s\n",
      "\n",
      "[LOAD]  ..\\..\\data\\defectors\\jit_bug_prediction_splits\\time\\val.parquet.gzip\n",
      "[SAVE]  ..\\..\\data\\defectors\\jit_bug_prediction_splits\\time\\val.feather\n",
      "→ done: load=0.096s, save=0.178s\n",
      "\n",
      "All conversions done.\n",
      "Total time: 119.83 seconds\n"
     ]
    }
   ],
   "source": [
    "def to_feather_path(parquet_path: Path) -> Path:\n",
    "    name = parquet_path.name\n",
    "\n",
    "    if name.endswith(\".parquet.gzip\"):\n",
    "        base = name[:-len(\".parquet.gzip\")]\n",
    "    elif name.endswith(\".parquet\"):\n",
    "        base = name[:-len(\".parquet\")]\n",
    "    else:\n",
    "        raise ValueError(\"File does not look like a parquet file: \" + name)\n",
    "\n",
    "    return parquet_path.with_name(base + \".feather\")\n",
    "\n",
    "def convert_parquet_to_feather(parquet_path: Path):\n",
    "    # feather_path = parquet_path.with_suffix(\".feather\")\n",
    "    feather_path = to_feather_path(parquet_path)\n",
    "\n",
    "\n",
    "    # Avoid overwriting if already converted\n",
    "    if feather_path.exists():\n",
    "        print(f\"[SKIP] {feather_path.name} already exists.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[LOAD]  {parquet_path}\")\n",
    "    start = time.perf_counter()\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    load_time = time.perf_counter() - start\n",
    "\n",
    "    print(f\"[SAVE]  {feather_path}\")\n",
    "    start = time.perf_counter()\n",
    "    df.to_feather(feather_path)\n",
    "    save_time = time.perf_counter() - start\n",
    "\n",
    "    print(f\"→ done: load={load_time:.3f}s, save={save_time:.3f}s\\n\")\n",
    "\n",
    "\n",
    "def find_all_parquets(root: Path):\n",
    "    \"\"\"Find .parquet or .parquet.gzip recursively.\"\"\"\n",
    "    for path in root.rglob(\"*\"):\n",
    "        if path.suffix in [\".parquet\", \".gzip\"] and \"parquet\" in path.name:\n",
    "            yield path\n",
    "\n",
    "start = time.perf_counter()\n",
    "print(f\"Searching for .parquet/.parquet.gzip in {DEFECTORS_DATA_DIR}\")\n",
    "parquet_files = list(find_all_parquets(DEFECTORS_DATA_DIR))\n",
    "\n",
    "print(f\"Found {len(parquet_files)} parquet files.\\n\")\n",
    "\n",
    "for p in parquet_files:\n",
    "    convert_parquet_to_feather(p)\n",
    "\n",
    "print(\"All conversions done.\")\n",
    "end = time.perf_counter()\n",
    "print(f\"Total time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8219667",
   "metadata": {},
   "source": [
    "### Exploring dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c250f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 feather datasets.\n",
      "\n",
      "\n",
      "=== ..\\..\\data\\defectors\\line_bug_prediction_splits\\random\\test.feather ===\n",
      "File size: 73.22 MB\n",
      "Rows: 10,000\n",
      "Columns: 7\n",
      "\n",
      "Column summary:\n",
      "                                          dtype  non_null  nulls\n",
      "datetime  datetime64[us, pytz.FixedOffset(240)]     10000      0\n",
      "commit                                   object     10000      0\n",
      "repo                                     object     10000      0\n",
      "filepath                                 object     10000      0\n",
      "content                                  object      9769    231\n",
      "methods                                  object     10000      0\n",
      "lines                                    object     10000      0\n",
      "\n",
      "Preview (first 3 rows):\n",
      "                   datetime                                    commit  \\\n",
      "1 2021-07-13 15:35:10+04:00  000fbe63d390c59b9c1e29216c35fc52b991f2f3   \n",
      "4 2022-07-11 13:12:55+04:00  038d5338530411bb47283fda1e84dec91137880b   \n",
      "7 2014-07-16 08:51:12+04:00  0786e84a33155ebc8d8d3502e3a7f3060b86a4ec   \n",
      "\n",
      "         repo                                           filepath  \\\n",
      "1   lightning  pytorch_lightning\\trainer\\connectors\\logger_co...   \n",
      "4  localstack                              localstack\\aws\\app.py   \n",
      "7      scrapy                          scrapy\\utils\\iterators.py   \n",
      "\n",
      "                                             content  \\\n",
      "1  b'# Copyright The PyTorch Lightning team.\\n#\\n...   \n",
      "4  b'import logging\\n\\nfrom localstack.aws import...   \n",
      "7  b'import re, csv, six\\n\\ntry:\\n    from cStrin...   \n",
      "\n",
      "                                     methods                lines  \n",
      "1  [extract_batch_size, _extract_batch_size]        [17, 24, 593]  \n",
      "4                                 [__init__]                 [62]  \n",
      "7                                  [csviter]  [3, 4, 5, 6, 7, 55]  \n",
      "\n",
      "=== ..\\..\\data\\defectors\\line_bug_prediction_splits\\random\\train.feather ===\n",
      "File size: 1444.81 MB\n",
      "Rows: 193,420\n",
      "Columns: 7\n",
      "\n",
      "Column summary:\n",
      "                                          dtype  non_null  nulls\n",
      "datetime  datetime64[us, pytz.FixedOffset(-60)]    193420      0\n",
      "commit                                   object    193420      0\n",
      "repo                                     object    193420      0\n",
      "filepath                                 object    193420      0\n",
      "content                                  object    190904   2516\n",
      "methods                                  object    193420      0\n",
      "lines                                    object    193420      0\n",
      "\n",
      "Preview (first 3 rows):\n",
      "                   datetime                                    commit  \\\n",
      "0 2016-08-11 14:07:40-01:00  01b498ec5109da22bf1b79d86efaecf45426ad51   \n",
      "0 2020-07-24 17:47:46-01:00  007bc310840d9cd5b37983a0c6ba82bd9e551c26   \n",
      "0 2014-07-16 03:51:12-01:00  0786e84a33155ebc8d8d3502e3a7f3060b86a4ec   \n",
      "\n",
      "                    repo                          filepath  \\\n",
      "0  django-rest-framework         rest_framework\\schemas.py   \n",
      "0                 poetry         poetry\\inspection\\info.py   \n",
      "0                 scrapy  scrapy\\contrib\\pipeline\\files.py   \n",
      "\n",
      "                                             content  \\\n",
      "0  b'from importlib import import_module\\n\\nfrom ...   \n",
      "0  b'import glob\\nimport logging\\nimport os\\nimpo...   \n",
      "0  b'\"\"\"\\nFiles Pipeline\\n\"\"\"\\n\\nimport hashlib\\n...   \n",
      "\n",
      "                                             methods  \\\n",
      "0  [insert_into, add_categories, get_api_endpoint...   \n",
      "0  [_find_dist_info, from_directory, from_sdist, ...   \n",
      "0                                  [file_downloaded]   \n",
      "\n",
      "                                               lines  \n",
      "0  [69, 87, 89, 92, 93, 95, 96, 97, 98, 99, 100, ...  \n",
      "0  [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 2...  \n",
      "0                          [14, 15, 16, 17, 18, 264]  \n",
      "\n",
      "=== ..\\..\\data\\defectors\\line_bug_prediction_splits\\random\\val.feather ===\n",
      "File size: 75.86 MB\n",
      "Rows: 10,000\n",
      "Columns: 7\n",
      "\n",
      "Column summary:\n",
      "                                          dtype  non_null  nulls\n",
      "datetime  datetime64[us, pytz.FixedOffset(480)]     10000      0\n",
      "commit                                   object     10000      0\n",
      "repo                                     object     10000      0\n",
      "filepath                                 object     10000      0\n",
      "content                                  object      9782    218\n",
      "methods                                  object     10000      0\n",
      "lines                                    object     10000      0\n",
      "\n",
      "Preview (first 3 rows):\n",
      "                    datetime                                    commit  \\\n",
      "0  2019-11-20 11:02:06+08:00  002a89cefada8894adf9717e83bde663ad1a54aa   \n",
      "2  2021-11-05 02:24:25+08:00  0155548384e90597f140560004364633430070cd   \n",
      "12 2018-12-10 16:15:57+08:00  0b40a7badf82c53c8a23b3a03273619f8440855d   \n",
      "\n",
      "      repo                       filepath  \\\n",
      "0   pandas  pandas\\core\\reshape\\concat.py   \n",
      "2   yolov5                     hubconf.py   \n",
      "12   black                      blackd.py   \n",
      "\n",
      "                                              content  \\\n",
      "0   b'\"\"\"\\nconcat routines\\n\"\"\"\\n\\nfrom typing imp...   \n",
      "2   b'# YOLOv5 \\xf0\\x9f\\x9a\\x80 by Ultralytics, GP...   \n",
      "12  b'import asyncio\\nfrom concurrent.futures impo...   \n",
      "\n",
      "                                              methods  \\\n",
      "0   [_get_frame_result_type, _get_comb_axis, _get_...   \n",
      "2                                           [_create]   \n",
      "12                                         [make_app]   \n",
      "\n",
      "                                                lines  \n",
      "0                   [5, 441, 447, 477, 524, 530, 541]  \n",
      "2                                  [31, 33, 128, 129]  \n",
      "12  [7, 21, 22, 23, 24, 25, 26, 27, 28, 29, 48, 49...  \n",
      "\n",
      "=== ..\\..\\data\\defectors\\line_bug_prediction_splits\\time\\test.feather ===\n",
      "File size: 74.71 MB\n",
      "Rows: 10,000\n",
      "Columns: 7\n",
      "\n",
      "Column summary:\n",
      "                                          dtype  non_null  nulls\n",
      "datetime  datetime64[us, pytz.FixedOffset(-60)]     10000      0\n",
      "commit                                   object     10000      0\n",
      "repo                                     object     10000      0\n",
      "filepath                                 object     10000      0\n",
      "content                                  object      9956     44\n",
      "methods                                  object     10000      0\n",
      "lines                                    object     10000      0\n",
      "\n",
      "Preview (first 3 rows):\n",
      "                   datetime                                    commit  \\\n",
      "0 2022-09-19 18:18:48-01:00  033177254070e511b1be88366995a526b3c676c9   \n",
      "1 2022-10-06 08:46:15-01:00  01d05f66fe5a189209538650dce319b2f7e192ee   \n",
      "2 2022-10-06 08:46:15-01:00  01d05f66fe5a189209538650dce319b2f7e192ee   \n",
      "\n",
      "         repo                               filepath  \\\n",
      "0  localstack  localstack\\services\\dynamodb\\utils.py   \n",
      "1   openpilot          selfdrive\\locationd\\laikad.py   \n",
      "2   openpilot           selfdrive\\sensord\\pigeond.py   \n",
      "\n",
      "                                             content  \\\n",
      "0  b'import logging\\nimport re\\nfrom typing impor...   \n",
      "1  b'#!/usr/bin/env python3\\nimport json\\nimport ...   \n",
      "2  b'#!/usr/bin/env python3\\nimport sys\\nimport t...   \n",
      "\n",
      "                     methods                                lines  \n",
      "0       [find_existing_item]                                [104]  \n",
      "1                     [main]                                [331]  \n",
      "2  [main, initialize_pigeon]  [119, 201, 202, 203, 204, 243, 244]  \n",
      "\n",
      "=== ..\\..\\data\\defectors\\line_bug_prediction_splits\\time\\train.feather ===\n",
      "File size: 1380.38 MB\n",
      "Rows: 185,369\n",
      "Columns: 7\n",
      "\n",
      "Column summary:\n",
      "                                           dtype  non_null  nulls\n",
      "datetime  datetime64[us, pytz.FixedOffset(-120)]    185369      0\n",
      "commit                                    object    185369      0\n",
      "repo                                      object    185369      0\n",
      "filepath                                  object    185369      0\n",
      "content                                   object    182680   2689\n",
      "methods                                   object    185369      0\n",
      "lines                                     object    185369      0\n",
      "\n",
      "Preview (first 3 rows):\n",
      "                   datetime                                    commit  \\\n",
      "0 2021-11-04 16:24:25-02:00  0155548384e90597f140560004364633430070cd   \n",
      "0 2021-06-08 08:46:09-02:00  00e7e12a3a412ea386806d5d4eeaed345e912940   \n",
      "0 2017-12-14 19:27:04-02:00  bfb3e09d1d1dff3a8341dc7a43fb44f876f97a1a   \n",
      "\n",
      "        repo                       filepath  \\\n",
      "0     yolov5                      detect.py   \n",
      "0      black           src\\black\\linegen.py   \n",
      "0  freqtrade  freqtrade\\exchange\\bittrex.py   \n",
      "\n",
      "                                             content  \\\n",
      "0  b'# YOLOv5 \\xf0\\x9f\\x9a\\x80 by Ultralytics, GP...   \n",
      "0  b'\"\"\"\\nGenerating lines of code.\\n\"\"\"\\nfrom fu...   \n",
      "0  b'import logging\\nfrom typing import List, Dic...   \n",
      "\n",
      "                                             methods  \\\n",
      "0                                                 []   \n",
      "0                                     [visit_STRING]   \n",
      "0  [get_markets, get_ticker, get_order, get_balan...   \n",
      "\n",
      "                                               lines  \n",
      "0                                       [28, 29, 30]  \n",
      "0                                    [229, 230, 231]  \n",
      "0  [42, 43, 44, 45, 46, 47, 48, 49, 50, 59, 70, 8...  \n",
      "\n",
      "=== ..\\..\\data\\defectors\\line_bug_prediction_splits\\time\\val.feather ===\n",
      "File size: 72.60 MB\n",
      "Rows: 10,000\n",
      "Columns: 7\n",
      "\n",
      "Column summary:\n",
      "                                          dtype  non_null  nulls\n",
      "datetime  datetime64[us, pytz.FixedOffset(300)]     10000      0\n",
      "commit                                   object     10000      0\n",
      "repo                                     object     10000      0\n",
      "filepath                                 object     10000      0\n",
      "content                                  object      9768    232\n",
      "methods                                  object     10000      0\n",
      "lines                                    object     10000      0\n",
      "\n",
      "Preview (first 3 rows):\n",
      "                   datetime                                    commit    repo  \\\n",
      "1 2022-09-01 06:37:24+05:00  1695bee78980e8954b431ace0c9b2194aa7203ba  pipenv   \n",
      "2 2022-09-01 06:37:24+05:00  1695bee78980e8954b431ace0c9b2194aa7203ba  pipenv   \n",
      "3 2022-08-31 04:56:43+05:00  188d9c18728a4e5578a03c9aa95ebcef9dcd8c71  pipenv   \n",
      "\n",
      "                       filepath  \\\n",
      "1            pipenv\\resolver.py   \n",
      "2      pipenv\\utils\\resolver.py   \n",
      "3  pipenv\\utils\\dependencies.py   \n",
      "\n",
      "                                             content  \\\n",
      "1  b'import json\\nimport logging\\nimport os\\nimpo...   \n",
      "2  b'import contextlib\\nimport hashlib\\nimport os...   \n",
      "3  b'import os\\nfrom contextlib import contextman...   \n",
      "\n",
      "                                             methods  \\\n",
      "1                   [handle_parsed_args, get_parser]   \n",
      "2                                [venv_resolve_deps]   \n",
      "3  [get_constraints_from_deps.is_constraints, get...   \n",
      "\n",
      "                                               lines  \n",
      "1  [99, 100, 101, 102, 103, 104, 129, 130, 131, 1...  \n",
      "2         [1029, 1030, 1031, 1032, 1033, 1034, 1035]  \n",
      "3                  [7, 8, 9, 10, 276, 277, 278, 283]  \n",
      "\n",
      "=== ..\\..\\data\\defectors\\jit_bug_prediction_splits\\random\\test.feather ===\n",
      "File size: 7.30 MB\n",
      "Rows: 10,000\n",
      "Columns: 7\n",
      "\n",
      "Column summary:\n",
      "                                          dtype  non_null  nulls\n",
      "datetime  datetime64[us, pytz.FixedOffset(240)]     10000      0\n",
      "commit                                   object     10000      0\n",
      "repo                                     object     10000      0\n",
      "filepath                                 object     10000      0\n",
      "content                                  object     10000      0\n",
      "methods                                  object     10000      0\n",
      "lines                                    object     10000      0\n",
      "\n",
      "Preview (first 3 rows):\n",
      "                   datetime                                    commit  \\\n",
      "1 2021-07-13 15:35:10+04:00  000fbe63d390c59b9c1e29216c35fc52b991f2f3   \n",
      "4 2022-07-11 13:12:55+04:00  038d5338530411bb47283fda1e84dec91137880b   \n",
      "7 2014-07-16 08:51:12+04:00  0786e84a33155ebc8d8d3502e3a7f3060b86a4ec   \n",
      "\n",
      "         repo                                           filepath  \\\n",
      "1   lightning  pytorch_lightning\\trainer\\connectors\\logger_co...   \n",
      "4  localstack                              localstack\\aws\\app.py   \n",
      "7      scrapy                          scrapy\\utils\\iterators.py   \n",
      "\n",
      "                                             content  \\\n",
      "1  @@ -14,13 +14,14 @@\\n from collections.abc imp...   \n",
      "4  @@ -59,6 +59,7 @@ class LocalstackAwsGateway(G...   \n",
      "7  @@ -1,5 +1,10 @@\\n import re, csv, six\\n \\n+tr...   \n",
      "\n",
      "                                     methods                lines  \n",
      "1  [extract_batch_size, _extract_batch_size]        [17, 24, 593]  \n",
      "4                                 [__init__]                 [62]  \n",
      "7                                  [csviter]  [3, 4, 5, 6, 7, 55]  \n",
      "\n",
      "=== ..\\..\\data\\defectors\\jit_bug_prediction_splits\\random\\train.feather ===\n",
      "File size: 148.27 MB\n",
      "Rows: 193,420\n",
      "Columns: 7\n",
      "\n",
      "Column summary:\n",
      "                                          dtype  non_null  nulls\n",
      "datetime  datetime64[us, pytz.FixedOffset(-60)]    193420      0\n",
      "commit                                   object    193420      0\n",
      "repo                                     object    193420      0\n",
      "filepath                                 object    193420      0\n",
      "content                                  object    193420      0\n",
      "methods                                  object    193420      0\n",
      "lines                                    object    193420      0\n",
      "\n",
      "Preview (first 3 rows):\n",
      "                   datetime                                    commit  \\\n",
      "0 2016-08-11 14:07:40-01:00  01b498ec5109da22bf1b79d86efaecf45426ad51   \n",
      "0 2020-07-24 17:47:46-01:00  007bc310840d9cd5b37983a0c6ba82bd9e551c26   \n",
      "0 2014-07-16 03:51:12-01:00  0786e84a33155ebc8d8d3502e3a7f3060b86a4ec   \n",
      "\n",
      "                    repo                          filepath  \\\n",
      "0  django-rest-framework         rest_framework\\schemas.py   \n",
      "0                 poetry         poetry\\inspection\\info.py   \n",
      "0                 scrapy  scrapy\\contrib\\pipeline\\files.py   \n",
      "\n",
      "                                             content  \\\n",
      "0  @@ -30,24 +30,6 @@ def is_api_view(callback):\\...   \n",
      "0  @@ -30,6 +30,17 @@ from poetry.utils.toml_file...   \n",
      "0  @@ -11,6 +11,11 @@ from six.moves.urllib.parse...   \n",
      "\n",
      "                                             methods  \\\n",
      "0  [insert_into, add_categories, get_api_endpoint...   \n",
      "0  [_find_dist_info, from_directory, from_sdist, ...   \n",
      "0                                  [file_downloaded]   \n",
      "\n",
      "                                               lines  \n",
      "0  [69, 87, 89, 92, 93, 95, 96, 97, 98, 99, 100, ...  \n",
      "0  [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 2...  \n",
      "0                          [14, 15, 16, 17, 18, 264]  \n",
      "\n",
      "=== ..\\..\\data\\defectors\\jit_bug_prediction_splits\\random\\val.feather ===\n",
      "File size: 7.22 MB\n",
      "Rows: 10,000\n",
      "Columns: 7\n",
      "\n",
      "Column summary:\n",
      "                                          dtype  non_null  nulls\n",
      "datetime  datetime64[us, pytz.FixedOffset(480)]     10000      0\n",
      "commit                                   object     10000      0\n",
      "repo                                     object     10000      0\n",
      "filepath                                 object     10000      0\n",
      "content                                  object     10000      0\n",
      "methods                                  object     10000      0\n",
      "lines                                    object     10000      0\n",
      "\n",
      "Preview (first 3 rows):\n",
      "                    datetime                                    commit  \\\n",
      "0  2019-11-20 11:02:06+08:00  002a89cefada8894adf9717e83bde663ad1a54aa   \n",
      "2  2021-11-05 02:24:25+08:00  0155548384e90597f140560004364633430070cd   \n",
      "12 2018-12-10 16:15:57+08:00  0b40a7badf82c53c8a23b3a03273619f8440855d   \n",
      "\n",
      "      repo                       filepath  \\\n",
      "0   pandas  pandas\\core\\reshape\\concat.py   \n",
      "2   yolov5                     hubconf.py   \n",
      "12   black                      blackd.py   \n",
      "\n",
      "                                              content  \\\n",
      "0   @@ -2,6 +2,7 @@\\n concat routines\\n \"\"\"\\n \\n+f...   \n",
      "2   @@ -27,10 +27,10 @@ def _create(name, pretrain...   \n",
      "12  @@ -4,6 +4,7 @@ from functools import partial\\...   \n",
      "\n",
      "                                              methods  \\\n",
      "0   [_get_frame_result_type, _get_comb_axis, _get_...   \n",
      "2                                           [_create]   \n",
      "12                                         [make_app]   \n",
      "\n",
      "                                                lines  \n",
      "0                   [5, 441, 447, 477, 524, 530, 541]  \n",
      "2                                  [31, 33, 128, 129]  \n",
      "12  [7, 21, 22, 23, 24, 25, 26, 27, 28, 29, 48, 49...  \n",
      "\n",
      "=== ..\\..\\data\\defectors\\jit_bug_prediction_splits\\time\\test.feather ===\n",
      "File size: 6.56 MB\n",
      "Rows: 10,000\n",
      "Columns: 7\n",
      "\n",
      "Column summary:\n",
      "                                          dtype  non_null  nulls\n",
      "datetime  datetime64[us, pytz.FixedOffset(-60)]     10000      0\n",
      "commit                                   object     10000      0\n",
      "repo                                     object     10000      0\n",
      "filepath                                 object     10000      0\n",
      "content                                  object     10000      0\n",
      "methods                                  object     10000      0\n",
      "lines                                    object     10000      0\n",
      "\n",
      "Preview (first 3 rows):\n",
      "                   datetime                                    commit  \\\n",
      "0 2022-09-19 18:18:48-01:00  033177254070e511b1be88366995a526b3c676c9   \n",
      "1 2022-10-06 08:46:15-01:00  01d05f66fe5a189209538650dce319b2f7e192ee   \n",
      "2 2022-10-06 08:46:15-01:00  01d05f66fe5a189209538650dce319b2f7e192ee   \n",
      "\n",
      "         repo                               filepath  \\\n",
      "0  localstack  localstack\\services\\dynamodb\\utils.py   \n",
      "1   openpilot          selfdrive\\locationd\\laikad.py   \n",
      "2   openpilot           selfdrive\\sensord\\pigeond.py   \n",
      "\n",
      "                                             content  \\\n",
      "0  @@ -101,9 +101,7 @@ class ItemFinder:\\n       ...   \n",
      "1  @@ -328,7 +328,7 @@ class EphemerisSourceType(...   \n",
      "2  @@ -116,7 +116,7 @@ class TTYPigeon():\\n      ...   \n",
      "\n",
      "                     methods                                lines  \n",
      "0       [find_existing_item]                                [104]  \n",
      "1                     [main]                                [331]  \n",
      "2  [main, initialize_pigeon]  [119, 201, 202, 203, 204, 243, 244]  \n",
      "\n",
      "=== ..\\..\\data\\defectors\\jit_bug_prediction_splits\\time\\train.feather ===\n",
      "File size: 139.85 MB\n",
      "Rows: 185,369\n",
      "Columns: 7\n",
      "\n",
      "Column summary:\n",
      "                                           dtype  non_null  nulls\n",
      "datetime  datetime64[us, pytz.FixedOffset(-120)]    185369      0\n",
      "commit                                    object    185369      0\n",
      "repo                                      object    185369      0\n",
      "filepath                                  object    185369      0\n",
      "content                                   object    185369      0\n",
      "methods                                   object    185369      0\n",
      "lines                                     object    185369      0\n",
      "\n",
      "Preview (first 3 rows):\n",
      "                   datetime                                    commit  \\\n",
      "0 2021-11-04 16:24:25-02:00  0155548384e90597f140560004364633430070cd   \n",
      "0 2021-06-08 08:46:09-02:00  00e7e12a3a412ea386806d5d4eeaed345e912940   \n",
      "0 2017-12-14 19:27:04-02:00  bfb3e09d1d1dff3a8341dc7a43fb44f876f97a1a   \n",
      "\n",
      "        repo                       filepath  \\\n",
      "0     yolov5                      detect.py   \n",
      "0      black           src\\black\\linegen.py   \n",
      "0  freqtrade  freqtrade\\exchange\\bittrex.py   \n",
      "\n",
      "                                             content  \\\n",
      "0  @@ -25,8 +25,9 @@ ROOT = Path(os.path.relpath(...   \n",
      "0  @@ -226,8 +226,9 @@ class LineGenerator(Visito...   \n",
      "0  @@ -39,6 +39,15 @@ class Bittrex(Exchange):\\n ...   \n",
      "\n",
      "                                             methods  \\\n",
      "0                                                 []   \n",
      "0                                     [visit_STRING]   \n",
      "0  [get_markets, get_ticker, get_order, get_balan...   \n",
      "\n",
      "                                               lines  \n",
      "0                                       [28, 29, 30]  \n",
      "0                                    [229, 230, 231]  \n",
      "0  [42, 43, 44, 45, 46, 47, 48, 49, 50, 59, 70, 8...  \n",
      "\n",
      "=== ..\\..\\data\\defectors\\jit_bug_prediction_splits\\time\\val.feather ===\n",
      "File size: 6.60 MB\n",
      "Rows: 10,000\n",
      "Columns: 7\n",
      "\n",
      "Column summary:\n",
      "                                          dtype  non_null  nulls\n",
      "datetime  datetime64[us, pytz.FixedOffset(300)]     10000      0\n",
      "commit                                   object     10000      0\n",
      "repo                                     object     10000      0\n",
      "filepath                                 object     10000      0\n",
      "content                                  object     10000      0\n",
      "methods                                  object     10000      0\n",
      "lines                                    object     10000      0\n",
      "\n",
      "Preview (first 3 rows):\n",
      "                   datetime                                    commit    repo  \\\n",
      "1 2022-09-01 06:37:24+05:00  1695bee78980e8954b431ace0c9b2194aa7203ba  pipenv   \n",
      "2 2022-09-01 06:37:24+05:00  1695bee78980e8954b431ace0c9b2194aa7203ba  pipenv   \n",
      "3 2022-08-31 04:56:43+05:00  188d9c18728a4e5578a03c9aa95ebcef9dcd8c71  pipenv   \n",
      "\n",
      "                       filepath  \\\n",
      "1            pipenv\\resolver.py   \n",
      "2      pipenv\\utils\\resolver.py   \n",
      "3  pipenv\\utils\\dependencies.py   \n",
      "\n",
      "                                             content  \\\n",
      "1  @@ -96,6 +96,12 @@ def get_parser():\\n        ...   \n",
      "2  @@ -1026,10 +1026,15 @@ def venv_resolve_deps(...   \n",
      "3  @@ -4,7 +4,10 @@ from typing import Mapping, S...   \n",
      "\n",
      "                                             methods  \\\n",
      "1                   [handle_parsed_args, get_parser]   \n",
      "2                                [venv_resolve_deps]   \n",
      "3  [get_constraints_from_deps.is_constraints, get...   \n",
      "\n",
      "                                               lines  \n",
      "1  [99, 100, 101, 102, 103, 104, 129, 130, 131, 1...  \n",
      "2         [1029, 1030, 1031, 1032, 1033, 1034, 1035]  \n",
      "3                  [7, 8, 9, 10, 276, 277, 278, 283]  \n"
     ]
    }
   ],
   "source": [
    "def find_feather_files(root: Path):\n",
    "    \"\"\"Return all .feather files recursively.\"\"\"\n",
    "    return list(root.rglob(\"*.feather\"))\n",
    "\n",
    "\n",
    "def summarize_feather(path: Path):\n",
    "    print(f\"\\n=== {path} ===\")\n",
    "    print(f\"File size: {path.stat().st_size / (1024**2):.2f} MB\")\n",
    "\n",
    "    # Load\n",
    "    df = pd.read_feather(path)\n",
    "\n",
    "    # Basic shape\n",
    "    print(f\"Rows: {len(df):,}\")\n",
    "    print(f\"Columns: {len(df.columns)}\")\n",
    "\n",
    "    # Schema: dtype + non-null count\n",
    "    print(\"\\nColumn summary:\")\n",
    "    summary = (\n",
    "        df.dtypes.to_frame(\"dtype\")\n",
    "        .join(df.notnull().sum().to_frame(\"non_null\"))\n",
    "        .join(df.isnull().sum().to_frame(\"nulls\"))\n",
    "    )\n",
    "    print(summary)\n",
    "\n",
    "    # Optional: show first 3 rows as a preview\n",
    "    print(\"\\nPreview (first 3 rows):\")\n",
    "    print(df.head(3))\n",
    "\n",
    "\n",
    "# ---- RUN ----\n",
    "\n",
    "feather_files = find_feather_files(DEFECTORS_DATA_DIR)\n",
    "print(f\"Found {len(feather_files)} feather datasets.\\n\")\n",
    "\n",
    "for f in feather_files:\n",
    "    summarize_feather(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f4c7e7",
   "metadata": {},
   "source": [
    "### Opening feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9d1cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fth_df = pd.read_feather(FTH_DATASET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d3cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to open parquet: 8.17880654335022 seconds\n",
      "Index(['011ea55a8f1630842c67603ac601d4d7ef6ccef9',\n",
      "       'bd12c4bfe7da8dcb37db8d6b081a1aa5f2cb517c',\n",
      "       'e0e57b4beb9809883d5ef0df0d0367385b7c8aa3',\n",
      "       '257ac9d17581bb67c24e7148e7bab37fc28ec64c',\n",
      "       '60d4d5e1aaa9fde3cf541ee335e284d05e75679c'],\n",
      "      dtype='object', name='commit')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 185369 entries, 0 to 43564\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count   Dtype                                 \n",
      "---  ------    --------------   -----                                 \n",
      " 0   datetime  185369 non-null  datetime64[us, pytz.FixedOffset(-120)]\n",
      " 1   commit    185369 non-null  object                                \n",
      " 2   repo      185369 non-null  object                                \n",
      " 3   filepath  185369 non-null  object                                \n",
      " 4   content   182680 non-null  object                                \n",
      " 5   methods   185369 non-null  object                                \n",
      " 6   lines     185369 non-null  object                                \n",
      "dtypes: datetime64[us, pytz.FixedOffset(-120)](1), object(6)\n",
      "memory usage: 11.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "open_start = time.time()\n",
    "df = pd.read_pickle(DATASET_DIR / \"train.pkl\")\n",
    "\n",
    "open_end = time.time()\n",
    "print(f\"Time to open parquet: {open_end - open_start} seconds\")\n",
    "\n",
    "# print(df['commit'].head())       # show first rows\n",
    "# Count occurrences of each commit\n",
    "commit_counts = df['commit'].value_counts()\n",
    "\n",
    "# Filter commits that occur more than once\n",
    "duplicate_commits = commit_counts[commit_counts > 1].index\n",
    "\n",
    "# Get first 5 commits that occur more than once\n",
    "first_five_duplicates = duplicate_commits[:5]\n",
    "\n",
    "print(first_five_duplicates)\n",
    "print(df.info())   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
