{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8696134",
   "metadata": {},
   "source": [
    "# 03 - Checking Data Consistency\n",
    "\n",
    "The purpose of this document is to ensure there is **no Noisy Data** in the interim dataset as the output of the *ETL.py* script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e193ac2f",
   "metadata": {},
   "source": [
    "## Setting Up Project Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e1c9c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyter_init import setup\n",
    "\n",
    "setup()\n",
    "\n",
    "from src_code.config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b63078",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8dafc3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataframe with 7363 rows and 31 columns\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime                      datetime64[us, pytz.FixedOffset(-60)]\n",
       "commit                                                       object\n",
       "repo                                                         object\n",
       "filepath                                                     object\n",
       "content                                                      object\n",
       "methods                                                      object\n",
       "lines                                                        object\n",
       "author_email                                                 object\n",
       "canonical_datetime                              datetime64[ns, UTC]\n",
       "author_exp_pre                                                int64\n",
       "author_recent_activity_pre                                    int64\n",
       "label                                                         int64\n",
       "loc_added                                                     int64\n",
       "loc_deleted                                                   int64\n",
       "files_changed                                                 int64\n",
       "hunks_count                                                   int64\n",
       "msg_len                                                       int64\n",
       "has_fix_kw                                                    int64\n",
       "has_bug_kw                                                    int64\n",
       "ast_delta                                                     int64\n",
       "complexity_delta                                              int64\n",
       "max_func_change                                               int64\n",
       "time_since_last_change                                        int64\n",
       "todo                                                          int64\n",
       "fixme                                                         int64\n",
       "try                                                           int64\n",
       "except                                                        int64\n",
       "raise                                                         int64\n",
       "code_embed                                                   object\n",
       "msg_embed                                                    object\n",
       "recent_churn                                                  int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DF_PATH = ETL_MAPPINGS['test']['current_newest']\n",
    "\n",
    "# ---- LOAD ----\n",
    "df = pd.read_feather(DF_PATH)\n",
    "print(f\"Loaded dataframe with {len(df)} rows and {len(df.columns)} columns\\n\")\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6eb0e6",
   "metadata": {},
   "source": [
    "## Converting NumpyArray -> List\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f43f4dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the NumPy arrays back to Python lists\n",
    "for col in ['code_embed', 'msg_embed', 'methods', 'lines']:\n",
    "    # Use .apply(list) or .apply(lambda x: x.tolist()) for robustness\n",
    "    df[col] = df[col].apply(list)\n",
    "\n",
    "# print(df['content'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8972c09",
   "metadata": {},
   "source": [
    "## Missing Value Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfa4ad42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 1. Missing Values per Column\n",
      "|                            |   0 |\n",
      "|:---------------------------|----:|\n",
      "| datetime                   |   0 |\n",
      "| commit                     |   0 |\n",
      "| repo                       |   0 |\n",
      "| filepath                   |   0 |\n",
      "| content                    |   0 |\n",
      "| methods                    |   0 |\n",
      "| lines                      |   0 |\n",
      "| author_email               |   0 |\n",
      "| canonical_datetime         |   0 |\n",
      "| author_exp_pre             |   0 |\n",
      "| author_recent_activity_pre |   0 |\n",
      "| label                      |   0 |\n",
      "| loc_added                  |   0 |\n",
      "| loc_deleted                |   0 |\n",
      "| files_changed              |   0 |\n",
      "| hunks_count                |   0 |\n",
      "| msg_len                    |   0 |\n",
      "| has_fix_kw                 |   0 |\n",
      "| has_bug_kw                 |   0 |\n",
      "| ast_delta                  |   0 |\n",
      "| complexity_delta           |   0 |\n",
      "| max_func_change            |   0 |\n",
      "| time_since_last_change     |   0 |\n",
      "| todo                       |   0 |\n",
      "| fixme                      |   0 |\n",
      "| try                        |   0 |\n",
      "| except                     |   0 |\n",
      "| raise                      |   0 |\n",
      "| code_embed                 |   0 |\n",
      "| msg_embed                  |   0 |\n",
      "| recent_churn               |   0 |\n"
     ]
    }
   ],
   "source": [
    "print(\"## 1. Missing Values per Column\")\n",
    "nulls = df.isnull().sum().sort_values(ascending=False)\n",
    "print(nulls.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d9e187",
   "metadata": {},
   "source": [
    "## Primary Key Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "449493f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 2. Primary Key Uniqueness Check\n",
      "Duplicate key rows: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"## 2. Primary Key Uniqueness Check\")\n",
    "key_cols = [\"repo\", \"commit\", \"filepath\"]\n",
    "\n",
    "dupes = df.duplicated(subset=key_cols).sum()\n",
    "print(f\"Duplicate key rows: {dupes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c918fc9",
   "metadata": {},
   "source": [
    "## Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bb5e4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 3. Label Distribution\n",
      "|   label |   proportion |\n",
      "|--------:|-------------:|\n",
      "|       0 |     0.887546 |\n",
      "|       1 |     0.112454 |\n"
     ]
    }
   ],
   "source": [
    "print(\"## 3. Label Distribution\")\n",
    "print(df['label'].value_counts(normalize=True).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a438280c",
   "metadata": {},
   "source": [
    "## Repository Distribution (Imbalance Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "699093f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 4. Repository Distribution\n",
      "| repo    |   proportion |\n",
      "|:--------|-------------:|\n",
      "| sentry  |    0.317941  |\n",
      "| ray     |    0.257911  |\n",
      "| core    |    0.237811  |\n",
      "| airflow |    0.0885509 |\n",
      "| pandas  |    0.0670922 |\n",
      "| ansible |    0.030694  |\n"
     ]
    }
   ],
   "source": [
    "print(\"## 4. Repository Distribution\")\n",
    "repo_dist = df['repo'].value_counts(normalize=True)\n",
    "print(repo_dist.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6fa3c1",
   "metadata": {},
   "source": [
    "## Value Range Scan for Numeric Columns\n",
    "\n",
    "Automatically detects:\n",
    "\n",
    "- negatives where not allowed\n",
    "- max values\n",
    "- suspicious spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f87b9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 5. Numeric Column Range Scan\n",
      "|                            |   min |   median |          mean |    max |\n",
      "|:---------------------------|------:|---------:|--------------:|-------:|\n",
      "| author_exp_pre             |     0 |       19 |   40.0522     |    394 |\n",
      "| author_recent_activity_pre |     0 |       12 |   24.9315     |    222 |\n",
      "| label                      |     0 |        0 |    0.112454   |      1 |\n",
      "| loc_added                  |     0 |       31 |  109.042      |   1955 |\n",
      "| loc_deleted                |     0 |       58 |  139.087      |   1985 |\n",
      "| files_changed              |     0 |        6 |   18.3603     |    136 |\n",
      "| hunks_count                |     0 |       32 |   85.9383     |    826 |\n",
      "| msg_len                    |    16 |      128 |  229.56       |   8064 |\n",
      "| has_fix_kw                 |     0 |        0 |    0.21513    |      1 |\n",
      "| has_bug_kw                 |     0 |        0 |    0.0309656  |      1 |\n",
      "| ast_delta                  |     0 |       21 |  172.508      |  12002 |\n",
      "| complexity_delta           |     0 |        0 |    6.14491    |    348 |\n",
      "| max_func_change            |     0 |       71 |  116.678      |    838 |\n",
      "| time_since_last_change     |     0 |     1338 | 7441.59       | 659000 |\n",
      "| todo                       |     0 |        0 |    0.186609   |     17 |\n",
      "| fixme                      |     0 |        0 |    0.00203721 |      2 |\n",
      "| try                        |     0 |        0 |    4.34931    |   1422 |\n",
      "| except                     |     0 |        0 |    1.67513    |    256 |\n",
      "| raise                      |     0 |        0 |    0.711395   |     54 |\n",
      "| recent_churn               |     0 |      822 | 6575.01       | 135911 |\n"
     ]
    }
   ],
   "source": [
    "print(\"## 5. Numeric Column Range Scan\")\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "ranges = pd.DataFrame({\n",
    "    \"min\": df[num_cols].min(),\n",
    "    \"median\": df[num_cols].median(),\n",
    "    \"mean\": df[num_cols].mean(),\n",
    "    \"max\": df[num_cols].max()\n",
    "})\n",
    "\n",
    "print(ranges.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bb1367",
   "metadata": {},
   "source": [
    "## Check Columns Expected to Be Non-Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d899461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 6. Negative Value Check\n",
      "loc_added: 0 negative values\n",
      "loc_deleted: 0 negative values\n",
      "files_changed: 0 negative values\n",
      "hunks_count: 0 negative values\n",
      "msg_len: 0 negative values\n",
      "ast_delta: 0 negative values\n",
      "complexity_delta: 0 negative values\n",
      "max_func_change: 0 negative values\n",
      "author_exp_pre: 0 negative values\n",
      "author_recent_activity_pre: 0 negative values\n",
      "todo: 0 negative values\n",
      "fixme: 0 negative values\n",
      "try: 0 negative values\n",
      "except: 0 negative values\n",
      "raise: 0 negative values\n",
      "recent_churn: 0 negative values\n"
     ]
    }
   ],
   "source": [
    "non_negative_cols = [\n",
    "    \"loc_added\", \"loc_deleted\",\n",
    "    \"files_changed\", \"hunks_count\",\n",
    "    \"msg_len\", \"ast_delta\",\n",
    "    \"complexity_delta\", \"max_func_change\",\n",
    "    \"author_exp_pre\", \"author_recent_activity_pre\",\n",
    "    \"todo\", \"fixme\", \"try\", \"except\", \"raise\",\n",
    "    \"recent_churn\"\n",
    "]\n",
    "\n",
    "print(\"## 6. Negative Value Check\")\n",
    "for col in non_negative_cols:\n",
    "    bad = (df[col] < 0).sum()\n",
    "    print(f\"{col}: {bad} negative values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f57675",
   "metadata": {},
   "source": [
    "## Suspicious Feature Check: time_since_last_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5727505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 7. time_since_last_change Outliers\n",
      "Negative values: 0\n",
      "99.9% quantile: 346372.0\n",
      "Min: 0\n",
      "Max: 659000\n"
     ]
    }
   ],
   "source": [
    "print(\"## 7. time_since_last_change Outliers\")\n",
    "tslc = df[\"time_since_last_change\"]\n",
    "\n",
    "print(f\"Negative values: {(tslc < 0).sum()}\")\n",
    "print(f\"99.9% quantile: {tslc.quantile(0.999)}\")\n",
    "print(f\"Min: {tslc.min()}\")\n",
    "print(f\"Max: {tslc.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac89da7b",
   "metadata": {},
   "source": [
    "### Understanding the Feature\n",
    "\n",
    "*time_since_last_change = c.committed_date - last_time*\n",
    "\n",
    "Where:\n",
    "- c.committed_date = current commit timestamp (UNIX seconds)\n",
    "- last_time = timestamp of first parent commit\n",
    "\n",
    "So the feature = time difference between consecutive commits.\n",
    "\n",
    "This represents how much time passed between commits in a repo.\n",
    "\n",
    "### Why Negative Values?\n",
    "\n",
    "Meaning:\n",
    "\n",
    "- Some commits appear to be ~4.6 days negative (-396,818 sec)\n",
    "- Some commits appear to be ~77 days ahead (6.6M sec)\n",
    "\n",
    "This is expected when real Git data is used.\n",
    "\n",
    "Git timestamps can go backwards because:\n",
    "\n",
    "#### (1) Rebased or rewritten history\n",
    "\n",
    "During rebases, old commits appear “later” than newer ones.\n",
    "\n",
    "*WHY?*\n",
    "\n",
    "When you merge/rebase, Git does not reorder commits chronologically.\n",
    "Instead, it preserves the logical order of development.\n",
    "\n",
    "#### (2) Merge parents\n",
    "\n",
    "You use only the first parent:\n",
    "\n",
    "if c.parents:\n",
    "    last_time = c.parents[0].committed_date\n",
    "\n",
    "\n",
    "But merges may introduce non-linear time ordering.\n",
    "\n",
    "#### (3) Clock drift\n",
    "\n",
    "Different authors → different local machine clocks.\n",
    "\n",
    "#### (4) Shallow clones or incomplete history\n",
    "\n",
    "If the repo is shallow-fetched, parent commits may have weird timestamps.\n",
    "\n",
    "None of this indicates your extraction is wrong.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad362de1",
   "metadata": {},
   "source": [
    "## Binary Flag Integity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eaf20288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 8. Binary Columns Integrity\n",
      "has_fix_kw: 0 invalid values\n",
      "has_bug_kw: 0 invalid values\n"
     ]
    }
   ],
   "source": [
    "print(\"## 8. Binary Columns Integrity\")\n",
    "bin_cols = [\"has_fix_kw\", \"has_bug_kw\"]\n",
    "\n",
    "for col in bin_cols:\n",
    "    bad = df[~df[col].isin([0,1])]\n",
    "    print(f\"{col}: {len(bad)} invalid values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abf7f3a",
   "metadata": {},
   "source": [
    "## Embedding Consistency Check\n",
    "\n",
    "Ensure:\n",
    "\n",
    "- no None\n",
    "- all lists\n",
    "- identical dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36efd21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "## 9. Embedding Structural Checks\n",
      "code_embed None count: 0\n",
      "msg_embed None count: 0\n",
      "\n",
      "Non-list code_embed rows: 0\n",
      "Non-list msg_embed rows: 0\n",
      "\n",
      "Embedding dimensionality distribution:\n",
      "code_embed\n",
      "768    7363\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# # Convert the NumPy arrays back to Python lists\n",
    "# for col in ['code_embed', 'msg_embed']:\n",
    "#     # Use .apply(list) or .apply(lambda x: x.tolist()) for robustness\n",
    "#     df[col] = df[col].apply(list)\n",
    "\n",
    "print(type(df.loc[0, 'code_embed']))\n",
    "print(type(df.loc[0, 'msg_embed']))\n",
    "\n",
    "print(\"## 9. Embedding Structural Checks\")\n",
    "\n",
    "# None count\n",
    "print(\"code_embed None count:\", df['code_embed'].isna().sum())\n",
    "print(\"msg_embed None count:\", df['msg_embed'].isna().sum())\n",
    "\n",
    "# Check if all are lists\n",
    "print(\"\\nNon-list code_embed rows:\", (~df['code_embed'].apply(lambda x: isinstance(x, list))).sum())\n",
    "print(\"Non-list msg_embed rows:\", (~df['msg_embed'].apply(lambda x: isinstance(x, list))).sum())\n",
    "\n",
    "# Check dimensionality\n",
    "dims = df['code_embed'].apply(lambda x: len(x) if isinstance(x, list) else None)\n",
    "print(\"\\nEmbedding dimensionality distribution:\")\n",
    "print(dims.value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5372970",
   "metadata": {},
   "source": [
    "## Datetime Consistency\n",
    "\n",
    "Check for:\n",
    "\n",
    "- NaT values\n",
    "- ordering sanity (commit should not be older than file's previous record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "752e0559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 10. Datetime Columns Audit\n",
      "datetime: NaT count = 0\n",
      "datetime: min = 2022-06-15 08:05:34-01:00, max = 2022-11-03 19:53:36-01:00\n",
      "canonical_datetime: NaT count = 0\n",
      "canonical_datetime: min = 2022-06-15 08:45:47+00:00, max = 2022-11-03 00:00:15+00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"## 10. Datetime Columns Audit\")\n",
    "\n",
    "date_cols = [\"datetime\", \"canonical_datetime\"]\n",
    "\n",
    "for col in date_cols:\n",
    "    print(f\"{col}: NaT count = {df[col].isna().sum()}\")\n",
    "    print(f\"{col}: min = {df[col].min()}, max = {df[col].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76ca36d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 12. Columns With Only One Unique Value\n"
     ]
    }
   ],
   "source": [
    "print(\"## 12. Columns With Only One Unique Value\")\n",
    "\n",
    "for col in df.columns:\n",
    "    # Convert arrays to tuples for uniqueness check\n",
    "    if df[col].apply(lambda x: isinstance(x, (np.ndarray, list))).any():\n",
    "        # Convert each element to a tuple (or a string representation)\n",
    "        # print(col)\n",
    "        unique_values = df[col].apply(lambda x: tuple(x) if isinstance(x, (np.ndarray, list)) else x)\n",
    "    else:\n",
    "        unique_values = df[col]\n",
    "\n",
    "    if unique_values.nunique(dropna=True) == 1:\n",
    "        print(f\"⚠️ {col} has only one unique value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ef3f8",
   "metadata": {},
   "source": [
    "## Check Text Columns for Weirdness\n",
    "\n",
    "Empty strings? Too short? Too long?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6e31166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 13. Text Field Checks\n",
      "Empty content rows: 30\n",
      "count      7363.000000\n",
      "mean       1697.582371\n",
      "std        7573.354188\n",
      "min           0.000000\n",
      "25%         476.000000\n",
      "50%         835.000000\n",
      "75%        1766.000000\n",
      "max      618763.000000\n",
      "Name: content, dtype: float64\n",
      "\n",
      "Empty methods rows: 2077\n",
      "count    7363.000000\n",
      "mean        1.991987\n",
      "std         3.684710\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         1.000000\n",
      "75%         2.000000\n",
      "max       118.000000\n",
      "Name: methods, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"## 13. Text Field Checks\")\n",
    "\n",
    "if 'content' in df.columns:\n",
    "    print(\"Empty content rows:\", (df['content'].str.len() == 0).sum())\n",
    "    print(df['content'].str.len().describe())\n",
    "\n",
    "# Check 'methods' if it is a list column\n",
    "if 'methods' in df.columns:\n",
    "    # Use len() on the Python lists\n",
    "    print(\"\\nEmpty methods rows:\", (df['methods'].apply(len) == 0).sum())\n",
    "    print(df['methods'].apply(len).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934954c2",
   "metadata": {},
   "source": [
    "## Check For Impossible Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f17bad1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 14. Logical Consistency Checks\n",
      "msg_len outliers (msg_len <= 0): 0\n"
     ]
    }
   ],
   "source": [
    "print(\"## 14. Logical Consistency Checks\")\n",
    "\n",
    "# msg_len should match commit message length\n",
    "if \"msg_len\" in df.columns:\n",
    "    print(\"msg_len outliers (msg_len <= 0):\", (df['msg_len'] <= 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72432152",
   "metadata": {},
   "source": [
    "## Filepath Sanity\n",
    "Check for Windows vs POSIX weirdness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6652381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filepaths without / (unexpected after normalization): 13\n",
      "23           setup.py\n",
      "67           setup.py\n",
      "156          setup.py\n",
      "481          setup.py\n",
      "2228         setup.py\n",
      "2958         setup.py\n",
      "3014    versioneer.py\n",
      "5093         setup.py\n",
      "5434         setup.py\n",
      "5863         setup.py\n",
      "6103         setup.py\n",
      "6307         setup.py\n",
      "6308         setup.py\n",
      "Name: filepath, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Normalize all filepaths to use forward slashes\n",
    "df['filepath'] = df['filepath'].str.replace('\\\\', '/', regex=False)\n",
    "\n",
    "# Check again for paths without a slash\n",
    "bad_paths = df[~df['filepath'].str.contains('/')]\n",
    "print(\"Filepaths without / (unexpected after normalization):\", len(bad_paths))\n",
    "print(df.loc[~df['filepath'].str.contains('/'), 'filepath'].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e72a7b",
   "metadata": {},
   "source": [
    "## Check Recent Churn for Extreme Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3fd5421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 16. recent_churn Outlier Scan\n",
      "count      7363.000000\n",
      "mean       6575.006519\n",
      "std       16244.949940\n",
      "min           0.000000\n",
      "25%         100.000000\n",
      "50%         822.000000\n",
      "75%        3891.000000\n",
      "max      135911.000000\n",
      "Name: recent_churn, dtype: float64\n",
      "99.9% quantile: 127555.13000000094\n"
     ]
    }
   ],
   "source": [
    "print(\"## 16. recent_churn Outlier Scan\")\n",
    "print(df['recent_churn'].describe())\n",
    "print(\"99.9% quantile:\", df['recent_churn'].quantile(0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbf4792",
   "metadata": {},
   "source": [
    "## Check Distribution of Code Activity Keywords\n",
    "\n",
    "(todo, fixme, try/except/raise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa24f007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 17. Keyword Column Distributions\n",
      "|        |   count |       mean |        std |   min |   25% |   50% |   75% |   max |\n",
      "|:-------|--------:|-----------:|-----------:|------:|------:|------:|------:|------:|\n",
      "| todo   |    7363 | 0.186609   |  1.07322   |     0 |     0 |     0 |     0 |    17 |\n",
      "| fixme  |    7363 | 0.00203721 |  0.0480105 |     0 |     0 |     0 |     0 |     2 |\n",
      "| try    |    7363 | 4.34931    | 26.5048    |     0 |     0 |     0 |     2 |  1422 |\n",
      "| except |    7363 | 1.67513    | 10.9471    |     0 |     0 |     0 |     0 |   256 |\n",
      "| raise  |    7363 | 0.711395   |  3.47064   |     0 |     0 |     0 |     0 |    54 |\n"
     ]
    }
   ],
   "source": [
    "print(\"## 17. Keyword Column Distributions\")\n",
    "kw_cols = [\"todo\", \"fixme\", \"try\", \"except\", \"raise\"]\n",
    "\n",
    "print(df[kw_cols].describe().T.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07619a53",
   "metadata": {},
   "source": [
    "## Duplicate commit-message / code-embed lengths\n",
    "\n",
    "Check uniformity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a880db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code_embed\n",
      "768    7363\n",
      "Name: count, dtype: int64\n",
      "msg_embed\n",
      "768    7363\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "code_duplicates = df['code_embed'].apply(len).value_counts().head()\n",
    "msg_duplicates = df['msg_embed'].apply(len).value_counts().head()\n",
    "\n",
    "print(code_duplicates)\n",
    "print(msg_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8f4a7a",
   "metadata": {},
   "source": [
    "So all your embeddings being length 768 means:\n",
    "\n",
    "- The model you are using outputs a 768-dimensional vector\n",
    "- The embedding extraction process worked for every commit\n",
    "- No corrupted or empty embeddings\n",
    "- No input missing (no None, no NaN, no empty list, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa72d1",
   "metadata": {},
   "source": [
    "## File extension distribution\n",
    "This reveals if:\n",
    "\n",
    "- non-Python files contaminate AST/keyword metrics\n",
    "- certain extensions dominate bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "349f52b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ext\n",
       "py     7355\n",
       "pyi       8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ext'] = df['filepath'].str.split('.').str[-1]\n",
    "df['ext'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b506f",
   "metadata": {},
   "source": [
    "This does not mean the entire Pandas repo only contains .py files.\n",
    "It means:\n",
    "\n",
    "All diffs included in your dataset modify .py or .pyi files.\n",
    "\n",
    "The Defectors dataset:\n",
    "\n",
    "- extracts commits that add/remove lines regarded as “faulty”\n",
    "- focuses on logical/code changes, not text, docs, or build files\n",
    "- filters out non-source changes for consistency\n",
    "\n",
    "So your dataset is a filtered view of the repo, not the repo itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67ca73a",
   "metadata": {},
   "source": [
    "## Diff size sanity\n",
    "\n",
    "Large mismatches could indicate:\n",
    "\n",
    "- truncated diffs\n",
    "- non-standard diff formatting\n",
    "- metadata lines (prefix +++, ---, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b168c242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     7363.000000\n",
       "mean      -204.758115\n",
       "std        433.672633\n",
       "min      -3627.000000\n",
       "25%       -257.000000\n",
       "50%        -65.000000\n",
       "75%         -2.000000\n",
       "max      18363.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df[\"content\"].str.count(\"\\n\") - df[\"loc_added\"] - df[\"loc_deleted\"]).describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02708ab2",
   "metadata": {},
   "source": [
    "The discrepancy between content line counts and loc_added + loc_deleted is normal in real Git data.\n",
    "\n",
    "Reasons:\n",
    "\n",
    "1. Multi-line statements / code folding in diffs\n",
    "2. Partial line changes counted in hunks\n",
    "3. Files with removed lines only (content now shorter)\n",
    "4. Large diffs in a single commit skew the stats\n",
    "\n",
    "No indication of extraction errors — these are just properties of real commit diffs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DP_Thesis-sh7Ft33M",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
