{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c733e47b",
   "metadata": {},
   "source": [
    "# 02 - Data Format Benchmark Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4bcc0f",
   "metadata": {},
   "source": [
    "## Dependencies & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5d6e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pyarrow.feather as feather\n",
    "import os\n",
    "\n",
    "from jupyter_init import setup\n",
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd154b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['datetime', 'commit', 'repo', 'filepath', 'content', 'methods',\n",
      "       'lines'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from src_code.config import DEFECTORS_DIR\n",
    "\n",
    "RAW_DATASET_FILE = DEFECTORS_DIR / \"line_bug_prediction_splits/time/train.parquet.gzip\"\n",
    "\n",
    "# i need to find the first feather file where the raw dataset is stored\n",
    "FTH_DATASET_FILE = RAW_DATASET_FILE.with_suffix(\"\").with_suffix(\".feather\")\n",
    "\n",
    "DATASET_DIR = RAW_DATASET_FILE.parent\n",
    "OUTPUT_DIR = DATASET_DIR / \"bench_formats\"\n",
    "\n",
    "# load the raw dataset\n",
    "df = pd.read_parquet(RAW_DATASET_FILE)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723eeee3",
   "metadata": {},
   "source": [
    "## Parquet -> Feather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b23d896",
   "metadata": {},
   "source": [
    "### Why Feather?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a492bd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original .parquet.gzip...\n",
      "parquet(gzip) read        32.9374 seconds   (rows=185,369)\n",
      "\n",
      "Saving to other formats...\n",
      "\n",
      "Benchmarking load speeds:\n",
      "parquet (gzip)            25.0007 seconds   (rows=185,369)\n",
      "parquet (raw)             16.0259 seconds   (rows=185,369)\n",
      "feather                   12.6331 seconds   (rows=185,369)\n",
      "pickle                    15.2545 seconds   (rows=185,369)\n"
     ]
    }
   ],
   "source": [
    "def measure_time(label, func):\n",
    "    \"\"\"Measure read time.\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    df = func()\n",
    "    end = time.perf_counter()\n",
    "    print(f\"{label:<25} {end - start:.4f} seconds   (rows={len(df):,})\")\n",
    "    return df\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Loading original .parquet.gzip...\")\n",
    "df = measure_time(\"parquet(gzip) read\", lambda: pd.read_parquet(RAW_DATASET_FILE))\n",
    "\n",
    "# Save alternative formats\n",
    "print(\"\\nSaving to other formats...\")\n",
    "\n",
    "# 1) Feather\n",
    "feather_file = OUTPUT_DIR / \"train.feather\"\n",
    "feather.write_feather(df, feather_file)\n",
    "\n",
    "# 2) Parquet uncompressed\n",
    "parquet_fast = OUTPUT_DIR / \"train_uncompressed.parquet\"\n",
    "df.to_parquet(parquet_fast, compression=None)\n",
    "\n",
    "# 3) Pickle\n",
    "pickle_file = OUTPUT_DIR / \"train.pkl\"\n",
    "df.to_pickle(pickle_file)\n",
    "\n",
    "# 4) CSV (optional — slowest & largest)\n",
    "# csv_file = OUTPUT_DIR / \"train.csv\"\n",
    "# df.to_csv(csv_file, index=False)\n",
    "\n",
    "\n",
    "print(\"\\nBenchmarking load speeds:\")\n",
    "measure_time(\"parquet (gzip)\", lambda: pd.read_parquet(RAW_DATASET_FILE))\n",
    "measure_time(\"parquet (raw)\", lambda: pd.read_parquet(parquet_fast))\n",
    "measure_time(\"feather\", lambda: feather.read_feather(feather_file))\n",
    "measure_time(\"pickle\", lambda: pd.read_pickle(pickle_file))\n",
    "# measure_time(\"csv\", lambda: pd.read_csv(csv_file))\n",
    "\n",
    "# deleting the bench formats\n",
    "for file in OUTPUT_DIR.iterdir():\n",
    "    os.remove(file)\n",
    "os.rmdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e7221",
   "metadata": {},
   "source": [
    "### Saving All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "929ab646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for .parquet/.parquet.gzip in C:\\Users\\fmojt\\Code\\Software Projects\\DiplomaThesis\\data\\raw\\defectors\n",
      "Found 12 parquet files.\n",
      "\n",
      "[SKIP] test.feather already exists.\n",
      "[SKIP] train.feather already exists.\n",
      "[SKIP] val.feather already exists.\n",
      "[SKIP] test.feather already exists.\n",
      "[SKIP] train.feather already exists.\n",
      "[SKIP] val.feather already exists.\n",
      "[SKIP] test.feather already exists.\n",
      "[SKIP] train.feather already exists.\n",
      "[SKIP] val.feather already exists.\n",
      "[SKIP] test.feather already exists.\n",
      "[SKIP] train.feather already exists.\n",
      "[SKIP] val.feather already exists.\n",
      "All conversions done.\n",
      "Total time: 0.03 seconds\n"
     ]
    }
   ],
   "source": [
    "def to_feather_path(parquet_path: Path) -> Path:\n",
    "    name = parquet_path.name\n",
    "\n",
    "    if name.endswith(\".parquet.gzip\"):\n",
    "        base = name[:-len(\".parquet.gzip\")]\n",
    "    elif name.endswith(\".parquet\"):\n",
    "        base = name[:-len(\".parquet\")]\n",
    "    else:\n",
    "        raise ValueError(\"File does not look like a parquet file: \" + name)\n",
    "\n",
    "    return parquet_path.with_name(base + \".feather\")\n",
    "\n",
    "def convert_parquet_to_feather(parquet_path: Path):\n",
    "    # feather_path = parquet_path.with_suffix(\".feather\")\n",
    "    feather_path = to_feather_path(parquet_path)\n",
    "\n",
    "\n",
    "    # Avoid overwriting if already converted\n",
    "    if feather_path.exists():\n",
    "        print(f\"[SKIP] {feather_path.name} already exists.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[LOAD]  {parquet_path}\")\n",
    "    start = time.perf_counter()\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    load_time = time.perf_counter() - start\n",
    "\n",
    "    print(f\"[SAVE]  {feather_path}\")\n",
    "    start = time.perf_counter()\n",
    "    df.to_feather(feather_path)\n",
    "    save_time = time.perf_counter() - start\n",
    "\n",
    "    print(f\"→ done: load={load_time:.3f}s, save={save_time:.3f}s\\n\")\n",
    "\n",
    "\n",
    "def find_all_parquets(root: Path):\n",
    "    \"\"\"Find .parquet or .parquet.gzip recursively.\"\"\"\n",
    "    for path in root.rglob(\"*\"):\n",
    "        if path.suffix in [\".parquet\", \".gzip\"] and \"parquet\" in path.name:\n",
    "            yield path\n",
    "\n",
    "start = time.perf_counter()\n",
    "print(f\"Searching for .parquet/.parquet.gzip in {DEFECTORS_DIR}\")\n",
    "parquet_files = list(find_all_parquets(DEFECTORS_DIR))\n",
    "\n",
    "print(f\"Found {len(parquet_files)} parquet files.\\n\")\n",
    "\n",
    "for p in parquet_files:\n",
    "    convert_parquet_to_feather(p)\n",
    "\n",
    "print(\"All conversions done.\")\n",
    "end = time.perf_counter()\n",
    "print(f\"Total time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8219667",
   "metadata": {},
   "source": [
    "### Exploring dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c250f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 feather datasets.\n",
      "\n",
      "\n",
      "=== C:\\Users\\fmojt\\Code\\Software Projects\\DiplomaThesis\\data\\raw\\defectors\\line_bug_prediction_splits\\random\\test.feather ===\n",
      "File size: 73.22 MB\n",
      "Rows: 10,000\n",
      "Columns: 7\n",
      "\n",
      "Column summary:\n",
      "                                          dtype  non_null  nulls\n",
      "datetime  datetime64[us, pytz.FixedOffset(240)]     10000      0\n",
      "commit                                   object     10000      0\n",
      "repo                                     object     10000      0\n",
      "filepath                                 object     10000      0\n",
      "content                                  object      9769    231\n",
      "methods                                  object     10000      0\n",
      "lines                                    object     10000      0\n",
      "\n",
      "Preview (first 3 rows):\n",
      "                   datetime                                    commit  \\\n",
      "1 2021-07-13 15:35:10+04:00  000fbe63d390c59b9c1e29216c35fc52b991f2f3   \n",
      "4 2022-07-11 13:12:55+04:00  038d5338530411bb47283fda1e84dec91137880b   \n",
      "7 2014-07-16 08:51:12+04:00  0786e84a33155ebc8d8d3502e3a7f3060b86a4ec   \n",
      "\n",
      "         repo                                           filepath  \\\n",
      "1   lightning  pytorch_lightning\\trainer\\connectors\\logger_co...   \n",
      "4  localstack                              localstack\\aws\\app.py   \n",
      "7      scrapy                          scrapy\\utils\\iterators.py   \n",
      "\n",
      "                                             content  \\\n",
      "1  b'# Copyright The PyTorch Lightning team.\\n#\\n...   \n",
      "4  b'import logging\\n\\nfrom localstack.aws import...   \n",
      "7  b'import re, csv, six\\n\\ntry:\\n    from cStrin...   \n",
      "\n",
      "                                     methods                lines  \n",
      "1  [extract_batch_size, _extract_batch_size]        [17, 24, 593]  \n",
      "4                                 [__init__]                 [62]  \n",
      "7                                  [csviter]  [3, 4, 5, 6, 7, 55]  \n",
      "\n",
      "=== C:\\Users\\fmojt\\Code\\Software Projects\\DiplomaThesis\\data\\raw\\defectors\\line_bug_prediction_splits\\random\\train.feather ===\n",
      "File size: 1444.81 MB\n",
      "Rows: 193,420\n",
      "Columns: 7\n",
      "\n",
      "Column summary:\n",
      "                                          dtype  non_null  nulls\n",
      "datetime  datetime64[us, pytz.FixedOffset(-60)]    193420      0\n",
      "commit                                   object    193420      0\n",
      "repo                                     object    193420      0\n",
      "filepath                                 object    193420      0\n",
      "content                                  object    190904   2516\n",
      "methods                                  object    193420      0\n",
      "lines                                    object    193420      0\n",
      "\n",
      "Preview (first 3 rows):\n",
      "                   datetime                                    commit  \\\n",
      "0 2016-08-11 14:07:40-01:00  01b498ec5109da22bf1b79d86efaecf45426ad51   \n",
      "0 2020-07-24 17:47:46-01:00  007bc310840d9cd5b37983a0c6ba82bd9e551c26   \n",
      "0 2014-07-16 03:51:12-01:00  0786e84a33155ebc8d8d3502e3a7f3060b86a4ec   \n",
      "\n",
      "                    repo                          filepath  \\\n",
      "0  django-rest-framework         rest_framework\\schemas.py   \n",
      "0                 poetry         poetry\\inspection\\info.py   \n",
      "0                 scrapy  scrapy\\contrib\\pipeline\\files.py   \n",
      "\n",
      "                                             content  \\\n",
      "0  b'from importlib import import_module\\n\\nfrom ...   \n",
      "0  b'import glob\\nimport logging\\nimport os\\nimpo...   \n",
      "0  b'\"\"\"\\nFiles Pipeline\\n\"\"\"\\n\\nimport hashlib\\n...   \n",
      "\n",
      "                                             methods  \\\n",
      "0  [insert_into, add_categories, get_api_endpoint...   \n",
      "0  [_find_dist_info, from_directory, from_sdist, ...   \n",
      "0                                  [file_downloaded]   \n",
      "\n",
      "                                               lines  \n",
      "0  [69, 87, 89, 92, 93, 95, 96, 97, 98, 99, 100, ...  \n",
      "0  [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 2...  \n",
      "0                          [14, 15, 16, 17, 18, 264]  \n",
      "\n",
      "=== C:\\Users\\fmojt\\Code\\Software Projects\\DiplomaThesis\\data\\raw\\defectors\\line_bug_prediction_splits\\random\\val.feather ===\n",
      "File size: 75.86 MB\n",
      "Rows: 10,000\n",
      "Columns: 7\n",
      "\n",
      "Column summary:\n",
      "                                          dtype  non_null  nulls\n",
      "datetime  datetime64[us, pytz.FixedOffset(480)]     10000      0\n",
      "commit                                   object     10000      0\n",
      "repo                                     object     10000      0\n",
      "filepath                                 object     10000      0\n",
      "content                                  object      9782    218\n",
      "methods                                  object     10000      0\n",
      "lines                                    object     10000      0\n",
      "\n",
      "Preview (first 3 rows):\n",
      "                    datetime                                    commit  \\\n",
      "0  2019-11-20 11:02:06+08:00  002a89cefada8894adf9717e83bde663ad1a54aa   \n",
      "2  2021-11-05 02:24:25+08:00  0155548384e90597f140560004364633430070cd   \n",
      "12 2018-12-10 16:15:57+08:00  0b40a7badf82c53c8a23b3a03273619f8440855d   \n",
      "\n",
      "      repo                       filepath  \\\n",
      "0   pandas  pandas\\core\\reshape\\concat.py   \n",
      "2   yolov5                     hubconf.py   \n",
      "12   black                      blackd.py   \n",
      "\n",
      "                                              content  \\\n",
      "0   b'\"\"\"\\nconcat routines\\n\"\"\"\\n\\nfrom typing imp...   \n",
      "2   b'# YOLOv5 \\xf0\\x9f\\x9a\\x80 by Ultralytics, GP...   \n",
      "12  b'import asyncio\\nfrom concurrent.futures impo...   \n",
      "\n",
      "                                              methods  \\\n",
      "0   [_get_frame_result_type, _get_comb_axis, _get_...   \n",
      "2                                           [_create]   \n",
      "12                                         [make_app]   \n",
      "\n",
      "                                                lines  \n",
      "0                   [5, 441, 447, 477, 524, 530, 541]  \n",
      "2                                  [31, 33, 128, 129]  \n",
      "12  [7, 21, 22, 23, 24, 25, 26, 27, 28, 29, 48, 49...  \n",
      "\n",
      "=== C:\\Users\\fmojt\\Code\\Software Projects\\DiplomaThesis\\data\\raw\\defectors\\line_bug_prediction_splits\\time\\test.feather ===\n",
      "File size: 74.71 MB\n",
      "Rows: 10,000\n",
      "Columns: 7\n",
      "\n",
      "Column summary:\n",
      "                                          dtype  non_null  nulls\n",
      "datetime  datetime64[us, pytz.FixedOffset(-60)]     10000      0\n",
      "commit                                   object     10000      0\n",
      "repo                                     object     10000      0\n",
      "filepath                                 object     10000      0\n",
      "content                                  object      9956     44\n",
      "methods                                  object     10000      0\n",
      "lines                                    object     10000      0\n",
      "\n",
      "Preview (first 3 rows):\n",
      "                   datetime                                    commit  \\\n",
      "0 2022-09-19 18:18:48-01:00  033177254070e511b1be88366995a526b3c676c9   \n",
      "1 2022-10-06 08:46:15-01:00  01d05f66fe5a189209538650dce319b2f7e192ee   \n",
      "2 2022-10-06 08:46:15-01:00  01d05f66fe5a189209538650dce319b2f7e192ee   \n",
      "\n",
      "         repo                               filepath  \\\n",
      "0  localstack  localstack\\services\\dynamodb\\utils.py   \n",
      "1   openpilot          selfdrive\\locationd\\laikad.py   \n",
      "2   openpilot           selfdrive\\sensord\\pigeond.py   \n",
      "\n",
      "                                             content  \\\n",
      "0  b'import logging\\nimport re\\nfrom typing impor...   \n",
      "1  b'#!/usr/bin/env python3\\nimport json\\nimport ...   \n",
      "2  b'#!/usr/bin/env python3\\nimport sys\\nimport t...   \n",
      "\n",
      "                     methods                                lines  \n",
      "0       [find_existing_item]                                [104]  \n",
      "1                     [main]                                [331]  \n",
      "2  [main, initialize_pigeon]  [119, 201, 202, 203, 204, 243, 244]  \n",
      "\n",
      "=== C:\\Users\\fmojt\\Code\\Software Projects\\DiplomaThesis\\data\\raw\\defectors\\line_bug_prediction_splits\\time\\train.feather ===\n",
      "File size: 1380.38 MB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(feather_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m feather datasets.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m feather_files:\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[43msummarize_feather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36msummarize_feather\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath.stat().st_size\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m(\u001b[32m1024\u001b[39m**\u001b[32m2\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m MB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Load\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_feather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Basic shape\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmojt\\.virtualenvs\\src_code-O5IEtaYk\\Lib\\site-packages\\pandas\\io\\feather_format.py:123\u001b[39m, in \u001b[36mread_feather\u001b[39m\u001b[34m(path, columns, use_threads, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[32m    120\u001b[39m     path, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m, storage_options=storage_options, is_text=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    121\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m lib.no_default \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_string_dtype():\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfeather\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_feather\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandles\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m     pa_table = feather.read_table(\n\u001b[32m    128\u001b[39m         handles.handle, columns=columns, use_threads=\u001b[38;5;28mbool\u001b[39m(use_threads)\n\u001b[32m    129\u001b[39m     )\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arrow_table_to_pandas(pa_table, dtype_backend=dtype_backend)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmojt\\.virtualenvs\\src_code-O5IEtaYk\\Lib\\site-packages\\pyarrow\\feather.py:225\u001b[39m, in \u001b[36mread_feather\u001b[39m\u001b[34m(source, columns, use_threads, memory_map, **kwargs)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_feather\u001b[39m(source, columns=\u001b[38;5;28;01mNone\u001b[39;00m, use_threads=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    199\u001b[39m                  memory_map=\u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs):\n\u001b[32m    200\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[33;03m    Read a pandas.DataFrame from Feather format. To read as pyarrow.Table use\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[33;03m    feather.read_table.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    223\u001b[39m \u001b[33;03m        The contents of the Feather file as a pandas.DataFrame\u001b[39;00m\n\u001b[32m    224\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m)\u001b[49m.to_pandas(use_threads=use_threads, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmojt\\.virtualenvs\\src_code-O5IEtaYk\\Lib\\site-packages\\pyarrow\\feather.py:255\u001b[39m, in \u001b[36mread_table\u001b[39m\u001b[34m(source, columns, memory_map, use_threads)\u001b[39m\n\u001b[32m    251\u001b[39m reader = _feather.FeatherReader(\n\u001b[32m    252\u001b[39m     source, use_memory_map=memory_map, use_threads=use_threads)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(columns, Sequence):\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mColumns must be a sequence but, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    259\u001b[39m                     .format(\u001b[38;5;28mtype\u001b[39m(columns).\u001b[34m__name__\u001b[39m))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def find_feather_files(root: Path):\n",
    "    \"\"\"Return all .feather files recursively.\"\"\"\n",
    "    return list(root.rglob(\"*.feather\"))\n",
    "\n",
    "\n",
    "def summarize_feather(path: Path):\n",
    "    print(f\"\\n=== {path} ===\")\n",
    "    print(f\"File size: {path.stat().st_size / (1024**2):.2f} MB\")\n",
    "\n",
    "    # Load\n",
    "    df = pd.read_feather(path)\n",
    "\n",
    "    # Basic shape\n",
    "    print(f\"Rows: {len(df):,}\")\n",
    "    print(f\"Columns: {len(df.columns)}\")\n",
    "\n",
    "    # Schema: dtype + non-null count\n",
    "    print(\"\\nColumn summary:\")\n",
    "    summary = (\n",
    "        df.dtypes.to_frame(\"dtype\")\n",
    "        .join(df.notnull().sum().to_frame(\"non_null\"))\n",
    "        .join(df.isnull().sum().to_frame(\"nulls\"))\n",
    "    )\n",
    "    print(summary)\n",
    "\n",
    "    # Optional: show first 3 rows as a preview\n",
    "    print(\"\\nPreview (first 3 rows):\")\n",
    "    print(df.head(3))\n",
    "\n",
    "\n",
    "# ---- RUN ----\n",
    "\n",
    "feather_files = find_feather_files(DEFECTORS_DIR)\n",
    "print(f\"Found {len(feather_files)} feather datasets.\\n\")\n",
    "\n",
    "for f in feather_files:\n",
    "    summarize_feather(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f4c7e7",
   "metadata": {},
   "source": [
    "### Opening feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d1cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fth_df = pd.read_feather(FTH_DATASET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d3cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to open parquet: 8.17880654335022 seconds\n",
      "Index(['011ea55a8f1630842c67603ac601d4d7ef6ccef9',\n",
      "       'bd12c4bfe7da8dcb37db8d6b081a1aa5f2cb517c',\n",
      "       'e0e57b4beb9809883d5ef0df0d0367385b7c8aa3',\n",
      "       '257ac9d17581bb67c24e7148e7bab37fc28ec64c',\n",
      "       '60d4d5e1aaa9fde3cf541ee335e284d05e75679c'],\n",
      "      dtype='object', name='commit')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 185369 entries, 0 to 43564\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count   Dtype                                 \n",
      "---  ------    --------------   -----                                 \n",
      " 0   datetime  185369 non-null  datetime64[us, pytz.FixedOffset(-120)]\n",
      " 1   commit    185369 non-null  object                                \n",
      " 2   repo      185369 non-null  object                                \n",
      " 3   filepath  185369 non-null  object                                \n",
      " 4   content   182680 non-null  object                                \n",
      " 5   methods   185369 non-null  object                                \n",
      " 6   lines     185369 non-null  object                                \n",
      "dtypes: datetime64[us, pytz.FixedOffset(-120)](1), object(6)\n",
      "memory usage: 11.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "open_start = time.time()\n",
    "df = pd.read_pickle(DATASET_DIR / \"train.pkl\")\n",
    "\n",
    "open_end = time.time()\n",
    "print(f\"Time to open parquet: {open_end - open_start} seconds\")\n",
    "\n",
    "# print(df['commit'].head())       # show first rows\n",
    "# Count occurrences of each commit\n",
    "commit_counts = df['commit'].value_counts()\n",
    "\n",
    "# Filter commits that occur more than once\n",
    "duplicate_commits = commit_counts[commit_counts > 1].index\n",
    "\n",
    "# Get first 5 commits that occur more than once\n",
    "first_five_duplicates = duplicate_commits[:5]\n",
    "\n",
    "print(first_five_duplicates)\n",
    "print(df.info())   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src_code-O5IEtaYk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
