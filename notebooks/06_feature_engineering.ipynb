{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99e271e2",
   "metadata": {},
   "source": [
    "# 06 - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c840060a",
   "metadata": {},
   "source": [
    "## 6.1 - Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac819a1",
   "metadata": {},
   "source": [
    "### 6.1.1 Setting Up Project Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b4d4e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.info(\"Setting up root by appending the parent to the sys...\")\n",
    "from jupyter_init import setup\n",
    "\n",
    "setup()\n",
    "\n",
    "from src_code.config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27473490",
   "metadata": {},
   "source": [
    "### 6.1.2 Setting Up Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcadee95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Starting notebook: 06_feature_engineering (Session 192) ==================\n",
      "[ENGINEERING RESULT] Logging configured.\n"
     ]
    }
   ],
   "source": [
    "from notebooks.logging_config import setup_notebook_logging\n",
    "\n",
    "logger, log_start, log_check, log_result = setup_notebook_logging(label=\"ENGINEERING\")\n",
    "\n",
    "log_start(print_to_console=True)\n",
    "log_result(\"Logging configured.\", print_to_console=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae30bd10",
   "metadata": {},
   "source": [
    "### 6.1.3 Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c38c618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ENGINEERING RESULT] Loaded dataframe with 19 rows and 31 columns\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_check(\"Loading the dataset...\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "TRANSFORMED_DF = EXTRACTED_DATA_DIR / \"test_labeled_features_partial_copy.feather\"\n",
    "# PREPROCESSED_DF = PREPROCESSED_DATA_DIR / \"train_preprocessed.feather\"\n",
    "\n",
    "# ---- LOAD ----\n",
    "df = pd.read_feather(TRANSFORMED_DF)\n",
    "msg = f\"Loaded dataframe with {len(df)} rows and {len(df.columns)} columns\\n\"\n",
    "# print(msg)\n",
    "log_result(msg, print_to_console=True)\n",
    "\n",
    "# For large datasets\n",
    "pd.set_option('display.max_columns', 50)\n",
    "sns.set_theme(style=\"whitegrid\", context=\"notebook\", palette=\"muted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ac5a6",
   "metadata": {},
   "source": [
    "## 6.2 - Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a068d3a",
   "metadata": {},
   "source": [
    "### 6.2.1 - Create interaction / derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49420299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: churn ratio\n",
    "if \"loc_added\" in df.columns and \"loc_deleted\" in df.columns:\n",
    "    df[\"loc_churn_ratio\"] = df[\"loc_added\"] / (df[\"loc_deleted\"] + 1)  # avoid division by zero\n",
    "\n",
    "# Example: recent activity per experience\n",
    "if \"author_recent_activity_pre\" in df.columns and \"author_exp_pre\" in df.columns:\n",
    "    df[\"activity_per_exp\"] = df[\"author_recent_activity_pre\"] / (df[\"author_exp_pre\"] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624167b3",
   "metadata": {},
   "source": [
    "### 6.2.2 - Binning / categorical transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49f86945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    133207.000000\n",
       "mean          0.049997\n",
       "std           0.217940\n",
       "min           0.000000\n",
       "25%           0.000000\n",
       "50%           0.000000\n",
       "75%           0.000000\n",
       "max           1.000000\n",
       "Name: extreme_churn_flag, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: bucket commits by size\n",
    "if \"loc_added\" in df.columns:\n",
    "    # bins = [0, 10, 50, 200, 1000, np.inf]\n",
    "    # labels = [\"very_small\", \"small\", \"medium\", \"large\", \"very_large\"]\n",
    "    # df[\"loc_added_bucket\"] = pd.cut(df[\"loc_added\"], bins=bins, labels=labels)\n",
    "    bins = [0, 2.3, 3.9, 5.3, 7.0, np.inf]\n",
    "    labels = [\"very_small\", \"small\", \"medium\", \"large\", \"very_large\"]\n",
    "\n",
    "    df[\"loc_added_bucket\"] = pd.cut(df[\"loc_added\"], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Example: boolean feature for extreme churn\n",
    "if \"recent_churn\" in df.columns:\n",
    "    threshold = df[\"recent_churn\"].quantile(0.95)\n",
    "    df[\"extreme_churn_flag\"] = (df[\"recent_churn\"] > threshold).astype(int)\n",
    "\n",
    "df['extreme_churn_flag'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca5cac14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2880395446434688\n",
      "0.31243840429137076\n"
     ]
    }
   ],
   "source": [
    "df[\"loc_added_bucket_cat\"] = df[\"loc_added_bucket\"].cat.codes\n",
    "print(df[\"loc_added_bucket_cat\"].corr(df[\"label\"]))\n",
    "print(df['loc_added'].corr(df['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21468a5",
   "metadata": {},
   "source": [
    "#### why loc_add_bucket?\n",
    "\n",
    "Because ML models often perform better when very skewed numeric features are also represented in categorical (binned) form.\n",
    "\n",
    "✓ Models detect thresholds better\n",
    "\n",
    "Bug likelihood typically increases when a commit crosses certain “size” thresholds:\n",
    "\n",
    "- tiny commits (<10 LOC) rarely introduce bugs\n",
    "- medium commits (50–200 LOC) are more risky\n",
    "- huge commits (1000+ LOC) are extremely risky\n",
    "\n",
    "Binning makes these thresholds explicit rather than hidden inside a numeric feature.\n",
    "\n",
    "✓ Models become more robust to noise\n",
    "\n",
    "- Instead of memorizing exact values like “3.044522” (your log-transformed LOC),\n",
    "the model gets a stable category: \"small\".\n",
    "\n",
    "✓ Helps tree-based models (XGBoost, RF, LightGBM)\n",
    "\n",
    "Trees thrive on categorical thresholds.\n",
    "One-hot-encoded buckets give them interpretable splits.\n",
    "\n",
    "\n",
    "#### Why extreme_churn_flag?\n",
    "\n",
    "*recent_churn* = how many lines were changed recently in the project\n",
    "\n",
    "High churn = a project area under rapid change\n",
    "\n",
    "High churn is known in research to correlate with bug-inducing commits\n",
    "(rapidly changing files are less stable)\n",
    "\n",
    "So the idea is:\n",
    "- commits with huge previous churn → more likely to be unstable → possibly bug-inducing\n",
    "\n",
    "This is a domain-inspired feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38478fc",
   "metadata": {},
   "source": [
    "### 6.2.3 - Aggregate LINE_TOKEN_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db6ab617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.constants import LINE_TOKEN_FEATURES\n",
    "\n",
    "\n",
    "df[\"line_token_total\"] = df[LINE_TOKEN_FEATURES].sum(axis=1)\n",
    "\n",
    "# Optionally create ratios per total lines (if loc_added exists)\n",
    "if \"loc_added\" in df.columns:\n",
    "    for token in LINE_TOKEN_FEATURES:\n",
    "        df[f\"{token}_ratio\"] = df[token] / (df[\"loc_added\"] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135420d3",
   "metadata": {},
   "source": [
    "### 6.2.4 - Feature interactions (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8600196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction_features = [\"loc_added\", \"loc_deleted\", \"hunks_count\"]\n",
    "\n",
    "from notebooks.constants import INTERACTION_FEATURES\n",
    "\n",
    "\n",
    "for i in range(len(INTERACTION_FEATURES)):\n",
    "    for j in range(i+1, len(INTERACTION_FEATURES)):\n",
    "        f1 = INTERACTION_FEATURES[i]\n",
    "        f2 = INTERACTION_FEATURES[j]\n",
    "        df[f\"{f1}_x_{f2}\"] = df[f1] * df[f2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e741f66",
   "metadata": {},
   "source": [
    "## 6.3 - Summary of engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a16e39ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ENGINEERING RESULT] Engineered features: ['loc_churn_ratio', 'activity_per_exp', 'loc_added_bucket', 'extreme_churn_flag', 'line_token_total', 'todo_ratio', 'fixme_ratio', 'try_ratio', 'except_ratio', 'raise_ratio', 'loc_added_x_loc_deleted', 'loc_added_x_hunks_count', 'loc_deleted_x_hunks_count']\n"
     ]
    }
   ],
   "source": [
    "from notebooks.constants import ENGINEERED_FEATURES\n",
    "\n",
    "\n",
    "# engineered_cols = [c for c in df.columns if c not in NUMERIC_FEATURES + LINE_TOKEN_FEATURES]\n",
    "# msg = \"Engineered features:\", ENGINEERED_FEATURES\n",
    "\n",
    "log_result(f\"Engineered features: {ENGINEERED_FEATURES}\", print_to_console=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c671b",
   "metadata": {},
   "source": [
    "## 6.4 Save the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17f5293f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset saved to C:\\Users\\fmojt\\Code\\Software Projects\\DiplomaThesis\\data\\preprocessed\\train_engineered.feather\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = PREPROCESSED_DATA_DIR / \"train_engineered.feather\"\n",
    "df.to_feather(OUTPUT_PATH)\n",
    "\n",
    "print(f\"Preprocessed dataset saved to {OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src_code-O5IEtaYk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
