{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99e271e2",
   "metadata": {},
   "source": [
    "# 06 - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c840060a",
   "metadata": {},
   "source": [
    "## 6.1 - Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac819a1",
   "metadata": {},
   "source": [
    "### 6.1.1 Setting Up Project Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b4d4e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.info(\"Setting up root by appending the parent to the sys...\")\n",
    "from jupyter_init import setup\n",
    "\n",
    "setup()\n",
    "\n",
    "from src_code.config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27473490",
   "metadata": {},
   "source": [
    "### 6.1.2 Setting Up Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcadee95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Starting notebook: 06-feature-engineering (Session 966) ==================\n",
      "[ENGINEERING RESULT] Logging configured.\n"
     ]
    }
   ],
   "source": [
    "from notebooks.logging_config import setup_notebook_logging\n",
    "\n",
    "logger, log_start, log_check, log_result = setup_notebook_logging(label=\"ENGINEERING\")\n",
    "\n",
    "log_start(print_to_console=True)\n",
    "log_result(\"Logging configured.\", print_to_console=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae30bd10",
   "metadata": {},
   "source": [
    "### 6.1.3 Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c38c618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ENGINEERING CHECK] Loading the dataset...\n",
      "[ENGINEERING RESULT] Loaded dataframe with 139545 rows and 29 columns\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_check(\"Loading the dataset...\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "SUBSET: SubsetType = 'train'\n",
    "# TRANSFORMED_DF = EXTRACTED_DATA_DIR / \"test_labeled_features_partial_copy.feather\"\n",
    "# DF = PROCESSED_DATA_DIR / \"train_preprocessed.feather\"\n",
    "DF_PATH = ENGINEERING_MAPPINGS[SUBSET]['input']\n",
    "\n",
    "# ---- LOAD ----\n",
    "df = pd.read_feather(DF_PATH)\n",
    "msg = f\"Loaded dataframe with {len(df)} rows and {len(df.columns)} columns\\n\"\n",
    "# print(msg)\n",
    "log_result(msg, print_to_console=True)\n",
    "\n",
    "# For large datasets\n",
    "pd.set_option('display.max_columns', 50)\n",
    "sns.set_theme(style=\"whitegrid\", context=\"notebook\", palette=\"muted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ac5a6",
   "metadata": {},
   "source": [
    "## 6.2 - Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a068d3a",
   "metadata": {},
   "source": [
    "### 6.2.1 - Create interaction / derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49420299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: churn ratio\n",
    "if \"loc_added\" in df.columns and \"loc_deleted\" in df.columns:\n",
    "    df[\"loc_churn_ratio\"] = df[\"loc_added\"] / (df[\"loc_deleted\"] + 1)  # avoid division by zero\n",
    "\n",
    "# Example: recent activity per experience\n",
    "if \"author_recent_activity_pre\" in df.columns and \"author_exp_pre\" in df.columns:\n",
    "    df[\"activity_per_exp\"] = df[\"author_recent_activity_pre\"] / (df[\"author_exp_pre\"] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25b7e134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['winsorize__author_exp_pre', 'winsorize__author_recent_activity_pre',\n",
       "       'winsorize__loc_added', 'winsorize__loc_deleted',\n",
       "       'winsorize__files_changed', 'winsorize__hunks_count',\n",
       "       'winsorize__msg_len', 'winsorize__ast_delta',\n",
       "       'winsorize__complexity_delta', 'winsorize__max_func_change',\n",
       "       'winsorize__time_since_last_change', 'winsorize__recent_churn',\n",
       "       'log_tokens__todo', 'log_tokens__fixme', 'log_tokens__try',\n",
       "       'log_tokens__except', 'log_tokens__raise',\n",
       "       'log_numeric__author_exp_pre',\n",
       "       'log_numeric__author_recent_activity_pre', 'log_numeric__loc_added',\n",
       "       'log_numeric__loc_deleted', 'log_numeric__files_changed',\n",
       "       'log_numeric__hunks_count', 'log_numeric__msg_len',\n",
       "       'log_numeric__ast_delta', 'log_numeric__complexity_delta',\n",
       "       'log_numeric__max_func_change', 'log_numeric__time_since_last_change',\n",
       "       'log_numeric__recent_churn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624167b3",
   "metadata": {},
   "source": [
    "### 6.2.2 - Binning / categorical transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49f86945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: bucket commits by size\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from notebooks.transformers import QuantileThresholdFlag\n",
    "from sklearn import set_config\n",
    "import joblib\n",
    "\n",
    "set_config(transform_output='pandas')\n",
    "\n",
    "if \"log_numeric__loc_added\" in df.columns:\n",
    "    # bins = [0, 10, 50, 200, 1000, np.inf]\n",
    "    # labels = [\"very_small\", \"small\", \"medium\", \"large\", \"very_large\"]\n",
    "    # df[\"loc_added_bucket\"] = pd.cut(df[\"loc_added\"], bins=bins, labels=labels)\n",
    "    bins = [0, 2.3, 3.9, 5.3, 7.0, np.inf]\n",
    "    labels = [\"very_small\", \"small\", \"medium\", \"large\", \"very_large\"]\n",
    "\n",
    "    df[\"loc_added_bucket\"] = pd.cut(df[\"log_numeric__loc_added\"], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# # Example: boolean feature for extreme churn\n",
    "# if \"recent_churn\" in df.columns:\n",
    "#     threshold = df[\"recent_churn\"].quantile(0.95)\n",
    "#     df[\"extreme_churn_flag\"] = (df[\"recent_churn\"] > threshold).astype(int)\n",
    "\n",
    "# df['extreme_churn_flag'].describe()\n",
    "\n",
    "# You'd need to create a list of features for which you want to create a flag\n",
    "# EXTREME_FLAG_FEATURES = [\"recent_churn\"]\n",
    "\n",
    "\n",
    "# if SUBSET == 'train':\n",
    "#     log_check(\"Detected train subset. Creating new preprocessor...\", print_to_console=True)\n",
    "\n",
    "#     preprocessor = ColumnTransformer(\n",
    "#         # ... existing transformers ...\n",
    "#         transformers=[('extreme_flags', QuantileThresholdFlag(quantile=0.95), EXTREME_FLAG_FEATURES),\n",
    "#         ]\n",
    "#         # ...\n",
    "#     )\n",
    "\n",
    "#     preprocessor.fit(df)\n",
    "#     df = preprocessor.transform(df)\n",
    "#     joblib.dump(preprocessor, ENGINEERING_PREPROCESSOR)\n",
    "# elif SUBSET in ('test', 'validate'):\n",
    "#     log_check(\"Detected test subset. Loading fitted preprocessor...\", print_to_console=True)\n",
    "#     loaded_preprocessor = joblib.load(ENGINEERING_PREPROCESSOR)\n",
    "#     df = loaded_preprocessor.transform(df)\n",
    "# else:\n",
    "#     msg = \"Unknown subset value!\"\n",
    "#     logger.error(msg)\n",
    "#     raise ValueError(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca5cac14",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmojt\\.virtualenvs\\DP_Thesis-sh7Ft33M\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mloc_added_bucket_cat\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mloc_added_bucket\u001b[39m\u001b[33m\"\u001b[39m].cat.codes\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(df[\u001b[33m\"\u001b[39m\u001b[33mloc_added_bucket_cat\u001b[39m\u001b[33m\"\u001b[39m].corr(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(df[\u001b[33m'\u001b[39m\u001b[33mloc_added\u001b[39m\u001b[33m'\u001b[39m].corr(df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmojt\\.virtualenvs\\DP_Thesis-sh7Ft33M\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmojt\\.virtualenvs\\DP_Thesis-sh7Ft33M\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'label'"
     ]
    }
   ],
   "source": [
    "df[\"loc_added_bucket_cat\"] = df[\"loc_added_bucket\"].cat.codes\n",
    "print(df[\"loc_added_bucket_cat\"].corr(df[\"label\"]))\n",
    "print(df['loc_added'].corr(df['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21468a5",
   "metadata": {},
   "source": [
    "#### why loc_add_bucket?\n",
    "\n",
    "Because ML models often perform better when very skewed numeric features are also represented in categorical (binned) form.\n",
    "\n",
    "✓ Models detect thresholds better\n",
    "\n",
    "Bug likelihood typically increases when a commit crosses certain “size” thresholds:\n",
    "\n",
    "- tiny commits (<10 LOC) rarely introduce bugs\n",
    "- medium commits (50–200 LOC) are more risky\n",
    "- huge commits (1000+ LOC) are extremely risky\n",
    "\n",
    "Binning makes these thresholds explicit rather than hidden inside a numeric feature.\n",
    "\n",
    "✓ Models become more robust to noise\n",
    "\n",
    "- Instead of memorizing exact values like “3.044522” (your log-transformed LOC),\n",
    "the model gets a stable category: \"small\".\n",
    "\n",
    "✓ Helps tree-based models (XGBoost, RF, LightGBM)\n",
    "\n",
    "Trees thrive on categorical thresholds.\n",
    "One-hot-encoded buckets give them interpretable splits.\n",
    "\n",
    "\n",
    "#### Why extreme_churn_flag?\n",
    "\n",
    "*recent_churn* = how many lines were changed recently in the project\n",
    "\n",
    "High churn = a project area under rapid change\n",
    "\n",
    "High churn is known in research to correlate with bug-inducing commits\n",
    "(rapidly changing files are less stable)\n",
    "\n",
    "So the idea is:\n",
    "- commits with huge previous churn → more likely to be unstable → possibly bug-inducing\n",
    "\n",
    "This is a domain-inspired feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38478fc",
   "metadata": {},
   "source": [
    "### 6.2.3 - Aggregate LINE_TOKEN_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ab617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.constants import LINE_TOKEN_FEATURES\n",
    "\n",
    "\n",
    "df[\"line_token_total\"] = df[LINE_TOKEN_FEATURES].sum(axis=1)\n",
    "\n",
    "# Optionally create ratios per total lines (if loc_added exists)\n",
    "if \"loc_added\" in df.columns:\n",
    "    for token in LINE_TOKEN_FEATURES:\n",
    "        df[f\"{token}_ratio\"] = df[token] / (df[\"loc_added\"] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135420d3",
   "metadata": {},
   "source": [
    "### 6.2.4 - Feature interactions (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8600196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction_features = [\"loc_added\", \"loc_deleted\", \"hunks_count\"]\n",
    "\n",
    "from notebooks.constants import INTERACTION_FEATURES\n",
    "\n",
    "\n",
    "for i in range(len(INTERACTION_FEATURES)):\n",
    "    for j in range(i+1, len(INTERACTION_FEATURES)):\n",
    "        f1 = INTERACTION_FEATURES[i]\n",
    "        f2 = INTERACTION_FEATURES[j]\n",
    "        df[f\"{f1}_x_{f2}\"] = df[f1] * df[f2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e741f66",
   "metadata": {},
   "source": [
    "## 6.3 - Summary of engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e39ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ENGINEERING RESULT] Engineered features: ['loc_churn_ratio', 'activity_per_exp', 'loc_added_bucket', 'extreme_churn_flag', 'line_token_total', 'todo_ratio', 'fixme_ratio', 'try_ratio', 'except_ratio', 'raise_ratio', 'loc_added_x_loc_deleted', 'loc_added_x_hunks_count', 'loc_deleted_x_hunks_count']\n"
     ]
    }
   ],
   "source": [
    "from notebooks.constants import ENGINEERED_FEATURES\n",
    "\n",
    "\n",
    "# engineered_cols = [c for c in df.columns if c not in NUMERIC_FEATURES + LINE_TOKEN_FEATURES]\n",
    "# msg = \"Engineered features:\", ENGINEERED_FEATURES\n",
    "\n",
    "log_result(f\"Engineered features: {ENGINEERED_FEATURES}\", print_to_console=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c671b",
   "metadata": {},
   "source": [
    "## 6.4 Save the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f5293f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset saved to C:\\Users\\fmojt\\Code\\Software Projects\\DiplomaThesis\\data\\preprocessed\\train_engineered.feather\n"
     ]
    }
   ],
   "source": [
    "\n",
    "log_check(\"Saving preprocessed dataset...\")\n",
    "# OUTPUT_PATH = PROCESSED_DATA_DIR / \"train_engineered.feather\"\n",
    "OUTPUT_PATH = ENGINEERING_MAPPINGS[SUBSET]['output']\n",
    "df.to_feather(OUTPUT_PATH)\n",
    "\n",
    "log_result(f\"Preprocessed dataset saved to {OUTPUT_PATH}\", print_to_console=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DP_Thesis-sh7Ft33M",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
