@INPROCEEDINGS{9641090,
  author={Kumar, T.Shathish and Booba, B.},
  booktitle={2021 IEEE Mysore Sub Section International Conference (MysuruCon)}, 
  title={A Systematic Study on Machine Learning Techniques for Predicting Software Faults}, 
  year={2021},
  volume={},
  number={},
  pages={133-136},
  keywords={Measurement;Fault diagnosis;Systematics;Object oriented modeling;Computational modeling;Software algorithms;Machine learning;classification;evaluation model;fault prediction;faulty systems;machine learning;object-oriented;prediction metrics},
  doi={10.1109/MysuruCon52639.2021.9641090}}

@inproceedings{inproceedings,
author = {Zheng, Wei and Cai, Fengyao and Chen, Tengfei and Yang, Teng and Yang, Fengyu and Xiao, Peng},
year = {2023},
month = {05},
pages = {85-90},
title = {Defect Prediction Method Using Stable Learning},
doi = {10.1109/IPCV57033.2023.00023}
}

@INPROCEEDINGS{Assim:software_deffect_prediction,
  author={Assim, Marwa and Obeidat, Qasem and Hammad, Mustafa},
  booktitle={2020 International Conference on Data Analytics for Business and Industry: Way Towards a Sustainable Economy (ICDABI)}, 
  title={Software Defects Prediction using Machine Learning Algorithms}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  keywords={Software;Prediction algorithms;Software algorithms;Machine learning algorithms;Classification algorithms;Predictive models;Measurement;software defect prediction;software defect;prediction model;machine learning (ML);artificial neural networks (ANN);SMOreg Classifier},
  doi={10.1109/ICDABI51230.2020.9325677}
}

@Article{su15065517,
AUTHOR = {Khalid, Aimen and Badshah, Gran and Ayub, Nasir and Shiraz, Muhammad and Ghouse, Mohamed},
TITLE = {Software Defect Prediction Analysis Using Machine Learning Techniques},
JOURNAL = {Sustainability},
VOLUME = {15},
YEAR = {2023},
NUMBER = {6},
ARTICLE-NUMBER = {5517},
URL = {https://www.mdpi.com/2071-1050/15/6/5517},
ISSN = {2071-1050},
ABSTRACT = {There is always a desire for defect-free software in order to maintain software quality for customer satisfaction and to save testing expenses. As a result, we examined various known ML techniques and optimized ML techniques on a freely available data set. The purpose of the research was to improve the model performance in terms of accuracy and precision of the dataset compared to previous research. As previous investigations show, the accuracy can be further improved. For this purpose, we employed K-means clustering for the categorization of class labels. Further, we applied classification models to selected features. Particle Swarm Optimization is utilized to optimize ML models. We evaluated the performance of models through precision, accuracy, recall, f-measure, performance error metrics, and a confusion matrix. The results indicate that all the ML and optimized ML models achieve the maximum results; however, the SVM and optimized SVM models outperformed with the highest achieved accuracy, 99% and 99.80%, respectively. The accuracy of NB, Optimized NB, RF, Optimized RF and ensemble approaches are 93.90%, 93.80%, 98.70%, 99.50%, 98.80% and 97.60, respectively. In this way, we achieve maximum accuracy compared to previous studies, which was our goal.},
DOI = {10.3390/su15065517}
}

@INPROCEEDINGS{Delhine::ICDS,
  author={Delphine Immaculate, S. and Farida Begam, M. and Floramary, M.},
  booktitle={2019 International Conference on Data Science and Communication (IconDSC)}, 
  title={Software Bug Prediction Using Supervised Machine Learning Algorithms}, 
  year={2019},
  volume={},
  number={},
  pages={1-7},
  keywords={Computer bugs;Machine learning algorithms;Software;Predictive models;Prediction algorithms;Data models;Machine learning;bug;classifier;cross validation;defect;machine learning;software metrics},
  doi={10.1109/IconDSC.2019.8816965}
}

@article{Tran2019,
  author       = {Ha Manh Tran and Son Thanh Le and Sinh Van Nguyen and Phong Thanh Ho},
  title        = {An Analysis of Software Bug Reports Using Machine Learning Techniques},
  journal      = {SN Computer Science},
  volume       = {1},
  number       = {1},
  pages        = {4},
  year         = {2019},
  doi          = {10.1007/s42979-019-0004-1},
  url          = {https://doi.org/10.1007/s42979-019-0004-1},
  abstract     = {Bug tracking systems manage bug reports for assuring the quality of software products. A bug report (alsoreferred as trouble, problem, ticket or defect) contains several features for problem management and resolution purposes. Severity and priority are two essential features of a bug report that define the effect level and fixing order of the bug. Determining these features is challenging and depends heavily on human being, e.g., software developers or system operators, especially for assessing a large number of error and warning events occurring on software products or network services. This study first proposes a comparison of machine learning techniques for assessing severity and priority for software bug reports and then chooses an approach of using optimal decision trees, or random forest, for further investigation. This approach aims at constructing multiple decision trees based on the subsets of the existing bug dataset and features, and then selecting the best decision trees to assess the severity and priority of new bugs. The approach can be applied for detecting and forecasting faults in large, complex communication networks and distributed systems today. We have presented the applicability of random forest for bug report analysis and performed several experiments on software bug datasets obtained from open source bug tracking systems. Random forest yields an average accuracy score of 0.75 that can be sufficient for assisting system operators in determining these features. We have provided some analysis of the experimental results.},
  issn         = {2661-8907}
}

@INPROCEEDINGS{Samantaray,
  author={Samantaray, Roshan and Das, Himansu},
  booktitle={2023 6th International Conference on Information Systems and Computer Networks (ISCON)}, 
  title={Performance Analysis of Machine Learning Algorithms Using Bagging Ensemble Technique for Software Fault Prediction}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  keywords={Machine learning algorithms;Software algorithms;NASA;Support vector machine classification;Static VAr compensators;Gaussian processes;Prediction algorithms;Software Fault Prediction;Ensemble Learning;Machine Learning;Bagging;Classification;Decision Tree;Logistic Regression;Gaussian Naive Bayes;Support Vector Machine;K-Nearest Neighbors.},
  doi={10.1109/ISCON57294.2023.10111952}}

  @article{Hammad,
author = {Hammad, Mustafa and Alqaddoumi, Abdulla and Alobaidy, Hadeel and Almseidein, Khalil},
year = {2019},
month = {09},
pages = {},
title = {Predicting Software Faults Based on K-Nearest Neighbors Classification}
}

@article{jovanovic2006software,
  title={Software testing methods and techniques},
  author={Jovanovi{\'c}, Irena},
  journal={The IPSI BgD transactions on internet research},
  volume={30},
  year={2006}
}

@article{khan2012comparative,
  title={A comparative study of white box, black box and grey box testing techniques},
  author={Khan, Mohd Ehmer and Khan, Farmeena},
  journal={International Journal of Advanced Computer Science and Applications},
  volume={3},
  number={6},
  year={2012},
  publisher={Citeseer}
}

@article{luo2001software,
  title={Software testing techniques},
  author={Luo, Lu},
  journal={Institute for software research international Carnegie mellon university Pittsburgh, PA},
  volume={15232},
  number={1-19},
  pages={19},
  year={2001}
}

@article{Tasnim,
  title={Challenges in Software Testing: How to handle the Testing Challenges},
  author={Rahnuma Tasnim},
  year={2023},
  url={https://www.softwaretestingstuff.com/challenges-of-software-testing/}
}

@article{Garousi,
author = {Garousi, Vahid and Felderer, Michael and Kuhrmann, Marco and Herkiloğlu, Kadir and Eldh, Sigrid},
title = {Exploring the industry's challenges in software testing: An empirical study},
journal = {Journal of Software: Evolution and Process},
volume = {32},
number = {8},
pages = {e2251},
keywords = {challenges, opinion survey, software industry, software testing},
doi = {https://doi.org/10.1002/smr.2251},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2251},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/smr.2251},
note = {e2251 JSME-18-0181.R1},
abstract = {Abstract Context Software testing is an important and costly software engineering activity in the industry. Despite the efforts of the software testing research community in the last several decades, various studies show that still many practitioners in the industry report challenges in their software testing tasks. Objective To shed light on industry's challenges in software testing, we characterize and synthesize the challenges reported by practitioners. Such concrete challenges can then be used for a variety of purposes, eg, research collaborations between industry and academia. Method Our empirical research method is opinion survey. By designing an online survey, we solicited practitioners' opinions about their challenges in different testing activities. Our dataset includes data from 72 practitioners from eight different countries. Results Our results show that test management and test automation are considered the most challenging among all testing activities by practitioners. Our results also include a set of 104 concrete challenges in software testing that may need further investigations by the research community. Conclusion We conclude that the focal points of industrial work and academic research in software testing differ. Furthermore, the paper at hand provides valuable insights concerning practitioners' “pain” points and, thus, provides researchers with a source of important research topics of high practical relevance.},
year = {2020}
}

@article{NAJIHI2022775,
title = {Software Testing from an Agile and Traditional view},
journal = {Procedia Computer Science},
volume = {203},
pages = {775-782},
year = {2022},
note = {17th International Conference on Future Networks and Communications / 19th International Conference on Mobile Systems and Pervasive Computing / 12th International Conference on Sustainable Energy Information Technology (FNC/MobiSPC/SEIT 2022), August 9-11, 2022, Niagara Falls, Ontario, Canada},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.07.116},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922007219},
author = {Soukaina Najihi and Sakina Elhadi and Rachida Ait Abdelouahid and Abdelaziz Marzak},
keywords = {Software Development Life Cycle (SDLC), Software Testing, Agile Software Development, Traditional Software Development, DevOps, Continuous Integration, Continuous Development, Continuous Testing},
abstract = {Project management has long been a challenge for Information Technology (IT) organizations to learn how to manage projects effectively and efficiently while balancing the iron triangle: cost, time and scope without sacrificing quality. Agile project management (APM) and traditional project management (TPM) are two different approaches related to software development, the core tenet of TPM is that projects are relatively simple, predictable, and linear with well-defined boundaries, whereas APM has emerged as a new methodology for managing high-risk, time-sensitive and scope flexible projects. While it is undeniable that the use of agile project management is increasing, the traditional opposite side continues to exist. The agile approach has an impact on all phases of software development, and they must adapt to this new way of thinking. Software testing is also impacted as a fundamental link in the software development life cycle that ensures quality. Software testing is a vital component of software Quality Assurance (QA) since it serves as the final review of the specification, design, and code. Has software testing evolved along with the evolution of project management? Is there a difference between a traditional and an agile software testing? This paper answers these questions by comparing software testing in the two approaches.}
}

@article{geeksforgeekshowtowritetestcases,
  author = {GeeksForGeeks},
  title = {How to Write Test Cases – Software Testing},
  year = {2025},
  month = {03},
}

@inbook{glenfordtheartofsoftwaretesting,
  author = {Glenford J. Myers and Corey Sandler and Tom Badgett},
  publisher = {John Wiley and Sons, Ltd},
  isbn = {9781119202486},
  title = {The Psychology and Economics of Software Testing},
  booktitle = {The Art of Software Testing},
  chapter = {2},
  pages = {5-18},
  doi = {https://doi.org/10.1002/9781119202486.ch2},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119202486.ch2},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119202486.ch2},
  year = {2012},
  keywords = {Software testing, human psychology, application testing, programmers, testing, economics, black-box testing, white-box testing, economics strategies, testing principles},
  abstract = {Summary Software testing is a technical task, but it also involves some important considerations of economics and human psychology. The most important considerations in software testing are issues of psychology. One of the primary causes of poor application testing is the fact that most programmers begin with a false definition of the term; these definitions are upside down. Understanding the true definition of software testing can make a profound difference in the success of your efforts. Human beings tend to be highly goal-oriented, and establishing the proper goal has an important psychological effect on them. The myriad implications related to the varied distorted definitions of software testing give rise to psychology problems. It is often impractical, often impossible, to find all the errors in a program. This fundamental problem causes implications for the economics of testing. To combat the challenges associated with testing economics, there are particular strategies: black-box testing and white-box testing. Apart from discussing the psychology problems of testing, this chapter explores the testing economics strategies. It also introduces a set of vital testing principles or guidelines. Most of these principles may seem obvious, yet they are all too often overlooked.}
}

@article{Zhang2003,
  author    = {Du Zhang and Jeffrey J.P. Tsai},
  title     = {Machine Learning and Software Engineering},
  journal   = {Software Quality Journal},
  volume    = {11},
  number    = {2},
  pages     = {87--119},
  year      = {2003},
  month     = {June},
  doi       = {10.1023/A:1023760326768},
  url       = {https://doi.org/10.1023/A:1023760326768},
  abstract  = {Machine learning deals with the issue of how to build programs that improve their performance at some task through experience. Machine learning algorithms have proven to be of great practical value in a variety of application domains. They are particularly useful for (a) poorly understood problem domains where little knowledge exists for the humans to develop effective algorithms; (b) domains where there are large databases containing valuable implicit regularities to be discovered; or (c) domains where programs must adapt to changing conditions. Not surprisingly, the field of software engineering turns out to be a fertile ground where many software development and maintenance tasks could be formulated as learning problems and approached in terms of learning algorithms. This paper deals with the subject of applying machine learning in software engineering. In the paper, we first provide the characteristics and applicability of some frequently utilized machine learning algorithms. We then summarize and analyze the existing work and discuss some general issues in this niche area. Finally we offer some guidelines on applying machine learning methods to software engineering tasks and use some software development and maintenance tasks as examples to show how they can be formulated as learning problems and approached in terms of learning algorithms.},
  issn      = {1573-1367}
}

@misc{Coursesa:ML,
  author = {Coursesa Stuff},
  howpublished = {Website, \url{https://www.coursera.org/articles/what-is-machine-learning?msockid=134a9c53edef6543091f88dbecad645c}},
  title = {What Is Machine Learning? Definition, Types, and Examples},
  year = {2025},
  month = {02},
}

@article{hammouri2018software,
  title={Software bug prediction using machine learning approach},
  author={Hammouri, Awni and Hammad, Mustafa and Alnabhan, Mohammad and Alsarayrah, Fatima},
  journal={International journal of advanced computer science and applications},
  volume={9},
  number={2},
  year={2018},
  publisher={Science and Information (SAI) Organization Limited}
}

@misc{Kaplan:DC,
  author = {Stewart Kaplan},
  howpublished = {Website, \url{https://enjoymachinelearning.com/blog/how-defects-are-classified-in-software-testing/}},
  title = {Understanding How Defects are Classified in Software Testing [Boost Your QA Skills]},
  year = {2024},
  month = {12}
}

@INPROCEEDINGS{Baskeles:SOEE,
  author={Baskeles, Bilge and Turhan, Burak and Bener, Ayse},
  booktitle={2007 22nd international symposium on computer and information sciences}, 
  title={Software effort estimation using machine learning methods}, 
  year={2007},
  volume={},
  number={},
  pages={1-6},
  keywords={Learning systems;Programming;Scheduling;Neural networks;Parametric statistics;Software engineering;Regression tree analysis;Predictive models;Cost function;Regression analysis},
  doi={10.1109/ISCIS.2007.4456863}}

  @INPROCEEDINGS{Lachmann:ML,
  author={Lachmann, Remo and Schulze, Sandro and Nieke, Manuel and Seidl, Christoph and Schaefer, Ina},
  booktitle={2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)}, 
  title={System-Level Test Case Prioritization Using Machine Learning}, 
  year={2016},
  volume={},
  number={},
  pages={361-368},
  keywords={Testing;Support vector machines;Software;Training data;Natural languages;Dictionaries;Training;System-Level Testing;Black-Box Testing;Test Case Prioritization;Supervised Machine Learning},
  doi={10.1109/ICMLA.2016.0065}}

  @article{AZEEM2019115,
title = {Machine learning techniques for code smell detection: A systematic literature review and meta-analysis},
journal = {Information and Software Technology},
volume = {108},
pages = {115-138},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918302623},
author = {Muhammad Ilyas Azeem and Fabio Palomba and Lin Shi and Qing Wang},
keywords = {Code smells, Machine learning, Systematic literature review},
abstract = {Background: Code smells indicate suboptimal design or implementation choices in the source code that often lead it to be more change- and fault-prone. Researchers defined dozens of code smell detectors, which exploit different sources of information to support developers when diagnosing design flaws. Despite their good accuracy, previous work pointed out three important limitations that might preclude the use of code smell detectors in practice: (i) subjectiveness of developers with respect to code smells detected by such tools, (ii) scarce agreement between different detectors, and (iii) difficulties in finding good thresholds to be used for detection. To overcome these limitations, the use of machine learning techniques represents an ever increasing research area. Objective: While the research community carefully studied the methodologies applied by researchers when defining heuristic-based code smell detectors, there is still a noticeable lack of knowledge on how machine learning approaches have been adopted for code smell detection and whether there are points of improvement to allow a better detection of code smells. Our goal is to provide an overview and discuss the usage of machine learning approaches in the field of code smells. Method: This paper presents a Systematic Literature Review (SLR) on Machine Learning Techniques for Code Smell Detection. Our work considers papers published between 2000 and 2017. Starting from an initial set of 2456 papers, we found that 15 of them actually adopted machine learning approaches. We studied them under four different perspectives: (i) code smells considered, (ii) setup of machine learning approaches, (iii) design of the evaluation strategies, and (iv) a meta-analysis on the performance achieved by the models proposed so far. Results: The analyses performed show that God Class, Long Method, Functional Decomposition, and Spaghetti Code have been heavily considered in the literature. Decision Trees and Support Vector Machines are the most commonly used machine learning algorithms for code smell detection. Models based on a large set of independent variables have performed well. JRip and Random Forest are the most effective classifiers in terms of performance. The analyses also reveal the existence of several open issues and challenges that the research community should focus on in the future. Conclusion: Based on our findings, we argue that there is still room for the improvement of machine learning techniques in the context of code smell detection. The open issues emerged in this study can represent the input for researchers interested in developing more powerful techniques.}
}

@misc{harer2018automatedsoftwarevulnerabilitydetection,
      title={Automated software vulnerability detection with machine learning}, 
      author={Jacob A. Harer and Louis Y. Kim and Rebecca L. Russell and Onur Ozdemir and Leonard R. Kosta and Akshay Rangamani and Lei H. Hamilton and Gabriel I. Centeno and Jonathan R. Key and Paul M. Ellingwood and Erik Antelman and Alan Mackay and Marc W. McConley and Jeffrey M. Opper and Peter Chin and Tomo Lazovich},
      year={2018},
      eprint={1803.04497},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/1803.04497}, 
}

@article{AJORLOO2024111805,
title = {A systematic review of machine learning methods in software testing},
journal = {Applied Soft Computing},
volume = {162},
pages = {111805},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111805},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624005799},
author = {Sedighe Ajorloo and Amirhossein Jamarani and Mehdi Kashfi and Mostafa {Haghi Kashani} and Abbas Najafizadeh},
keywords = {Machine learning, Software testing, Quality of software, Systematic review},
abstract = {Background
The quest for higher software quality remains a paramount concern in software testing, prompting a shift towards leveraging machine learning techniques for enhanced testing efficacy.
Objective
The objective of this paper is to identify, categorize, and systematically compare the present studies on software testing utilizing machine learning methods.
Method
This study conducts a systematic literature review (SLR) of 40 pertinent studies spanning from 2018 to March 2024 to comprehensively analyze and classify machine learning methods in software testing. The review encompasses supervised learning, unsupervised learning, reinforcement learning, and hybrid learning approaches.
Results
The strengths and weaknesses of each reviewed paper are dissected in this study. This paper also provides an in-depth analysis of the merits of machine learning methods in the context of software testing and addresses current unresolved issues. Potential areas for future research have been discussed, and statistics of each review paper have been collected.
Conclusion
By addressing these aspects, this study contributes to advancing the discourse on machine learning's role in software testing and paves the way for substantial improvements in testing efficacy and software quality.}
}